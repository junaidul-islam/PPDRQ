{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55ba4ba",
   "metadata": {},
   "source": [
    "# PDRQ_Reliability_Enhancement\n",
    "Calibration: ECE, Brier, reliability diagrams for every model.\n",
    "results CSV, metric bar plots, cost-performance chart, λ-sensitivity sweep, confusion matrices, per-class reports, side-by-side reliability grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import math \n",
    "import random \n",
    "import time \n",
    "import warnings \n",
    "from typing import Dict, List, Tuple \n",
    " \n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from PIL import Image \n",
    "from sklearn.metrics import ( \n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report \n",
    ") \n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from torchvision import models, transforms \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm \n",
    "import pandas as pd \n",
    " \n",
    "warnings.filterwarnings(\"ignore\") \n",
    " \n",
    "# ------------------------------- \n",
    "# 1. Config \n",
    "# ------------------------------- \n",
    "class Config: \n",
    "    data_dir = 'dataset'              # expected: data/{train,validation,test,unseen}/{AD,MCI,NC} \n",
    "    output_dir = 'Save results here' \n",
    " \n",
    "    image_size = 299 \n",
    "    num_classes = 3                # AD / MCI / NC \n",
    " \n",
    "    batch_size = 16                # slightly smaller to stabilize BN \n",
    "    epochs = 60                    # regularized training \n",
    "    learning_rate = 3e-4 \n",
    "    weight_decay = 5e-4            # stronger regularization \n",
    "    patience_lr_scheduler = 3 \n",
    "    patience_early_stopping = 10 \n",
    "    grad_clip = 1.0 \n",
    " \n",
    "    # Triplet & PPDRQ \n",
    "    lambda_triplet = 0.1 \n",
    "    triplet_margin = 0.2 \n",
    "    epsilon_p = 0.1 \n",
    "    epsilon_n = 0.2 \n",
    " \n",
    "    # MC-Dropout & Ensemble \n",
    "    mc_dropout_passes = 30 \n",
    "    num_ensemble_models = 5 \n",
    " \n",
    "    # Regularization toggles \n",
    "    use_mixup_cutmix = True \n",
    "    mixup_alpha = 0.2 \n",
    "    cutmix_alpha = 0.2 \n",
    "    label_smoothing = 0.05 \n",
    " \n",
    "    # Data safety \n",
    "    allow_make_dummy = False       # IMPORTANT: keep False for real training \n",
    " \n",
    "    # Device \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    " \n",
    " \n",
    "def set_seed(seed: int = 42): \n",
    "    torch.manual_seed(seed) \n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    np.random.seed(seed) \n",
    "    random.seed(seed) \n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark = False \n",
    " \n",
    " \n",
    "os.makedirs(Config.output_dir, exist_ok=True) \n",
    "print(f\"Using device: {Config.device}\") \n",
    "set_seed(42) \n",
    " \n",
    "# ------------------------------- \n",
    "# 2. Dataset \n",
    "# ------------------------------- \n",
    "class MRIDataset(Dataset): \n",
    "    def __init__(self, data_dir: str, phase: str, transform=None): \n",
    "        self.data_dir = os.path.join(data_dir, phase) \n",
    "        self.transform = transform \n",
    "        self.class_to_idx = {'AD': 0, 'MCI': 1, 'NC': 2} \n",
    "        self.image_paths: List[str] = [] \n",
    "        self.labels: List[int] = [] \n",
    " \n",
    "        for cname, idx in self.class_to_idx.items(): \n",
    "            cdir = os.path.join(self.data_dir, cname) \n",
    "            if not os.path.exists(cdir): \n",
    "                if Config.allow_make_dummy: \n",
    "                    print(f\"[WARN] Missing {cdir}. Creating dummy images for demo only.\") \n",
    "                    os.makedirs(cdir, exist_ok=True) \n",
    "                    n = 8 if phase != 'train' else 64 \n",
    "                    for i in range(n): \n",
    "                        img = Image.new('L', (Config.image_size, Config.image_size), color=random.randint(0, 255)) \n",
    "                        img.convert('RGB').save(os.path.join(cdir, f'dummy_{i}.png')) \n",
    "                else: \n",
    "                    print(f\"[WARN] Missing class dir: {cdir}. Skipping.\") \n",
    "                    continue \n",
    " \n",
    "            files = [f for f in os.listdir(cdir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))] \n",
    "            if not files and Config.allow_make_dummy: \n",
    "                print(f\"[WARN] Empty {cdir}. Creating dummy images for demo only.\") \n",
    "                n = 8 if phase != 'train' else 64 \n",
    "                for i in range(n): \n",
    "                    img = Image.new('L', (Config.image_size, Config.image_size), color=random.randint(0, 255)) \n",
    "                    img.convert('RGB').save(os.path.join(cdir, f'dummy_{i}.png')) \n",
    "                files = [f for f in os.listdir(cdir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))] \n",
    " \n",
    "            for name in files: \n",
    "                self.image_paths.append(os.path.join(cdir, name)) \n",
    "                self.labels.append(idx) \n",
    " \n",
    "    def __len__(self): \n",
    "        return len(self.image_paths) \n",
    " \n",
    "    def __getitem__(self, idx: int): \n",
    "        p = self.image_paths[idx] \n",
    "        y = self.labels[idx] \n",
    "        img = Image.open(p).convert('RGB') \n",
    "        if self.transform: \n",
    "            img = self.transform(img) \n",
    "        return img, y, idx \n",
    " \n",
    "# Medically sensible augs (avoid strong color jitter for brain MRIs) \n",
    "train_tfms = transforms.Compose([ \n",
    "    transforms.Resize(int(Config.image_size*1.1)), \n",
    "    transforms.RandomResizedCrop(Config.image_size, scale=(0.8, 1.0), ratio=(0.9, 1.1)), \n",
    "    transforms.RandomRotation(10), \n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.GaussianBlur(kernel_size=3), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3), value='random') \n",
    "]) \n",
    " \n",
    "val_test_tfms = transforms.Compose([ \n",
    "    transforms.Resize((Config.image_size, Config.image_size)), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "]) \n",
    " \n",
    "train_dataset = MRIDataset(Config.data_dir, 'train', train_tfms) \n",
    "val_dataset   = MRIDataset(Config.data_dir, 'validation', val_test_tfms) \n",
    "test_dataset  = MRIDataset(Config.data_dir, 'test', val_test_tfms) \n",
    "unseen_dataset= MRIDataset(Config.data_dir, 'unseen', val_test_tfms) \n",
    " \n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}, Unseen: {len(unseen_dataset)}\") \n",
    "if len(train_dataset) == 0: \n",
    "    print(\"[ERROR] Train dataset is empty. Please prepare data under data/train/{AD,MCI,NC}.\") \n",
    " \n",
    "# Dataloaders (num_workers=0 for portability) \n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=0, drop_last=True) \n",
    "val_loader   = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=0) \n",
    "test_loader  = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=0) \n",
    "unseen_loader= DataLoader(unseen_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=0) \n",
    " \n",
    "# ------------------------------- \n",
    "# 3. Model \n",
    "# ------------------------------- \n",
    "class CustomInceptionV3(nn.Module): \n",
    "    def __init__(self, num_classes=3, dropout_rate=0.6, include_aux_logits=True, train_last_blocks_only=True): \n",
    "        super().__init__() \n",
    "        import torchvision \n",
    "        try: \n",
    "            # Torchvision requires aux_logits=True when using pretrained weights \n",
    "            self.inception_base = torchvision.models.inception_v3( \n",
    "                weights=torchvision.models.Inception_V3_Weights.IMAGENET1K_V1, \n",
    "                aux_logits=True \n",
    "            ) \n",
    "        except Exception: \n",
    "            self.inception_base = models.inception_v3(pretrained=True, aux_logits=True) \n",
    "        # Expose 2048-d features \n",
    "        self.inception_base.fc = nn.Identity() \n",
    " \n",
    "        if train_last_blocks_only: \n",
    "            for p in self.inception_base.parameters(): \n",
    "                p.requires_grad = False \n",
    "            for name, m in self.inception_base.named_modules(): \n",
    "                if name.startswith('Mixed_7'): \n",
    "                    for p in m.parameters(): \n",
    "                        p.requires_grad = True \n",
    " \n",
    "        self.bn = nn.BatchNorm1d(2048) \n",
    "        self.dropout = nn.Dropout(p=dropout_rate) \n",
    "        self.hidden1 = nn.Linear(2048, 1024) \n",
    "        self.hidden2 = nn.Linear(1024, 512) \n",
    "        self.fc_head = nn.Linear(2048, num_classes) \n",
    " \n",
    "        self.mixed_7c_output = None \n",
    "        def hook_fn(_m, _in, out): \n",
    "            self.mixed_7c_output = out \n",
    "        self.inception_base.Mixed_7c.register_forward_hook(hook_fn) \n",
    " \n",
    "    def _extract_features(self, x): \n",
    "        out = self.inception_base(x) \n",
    "        # In train mode with aux_logits=True, inception_v3 returns InceptionOutputs \n",
    "        if hasattr(out, 'logits'): \n",
    "            return out.logits \n",
    "        if isinstance(out, (tuple, list)) and len(out) > 0: \n",
    "            return out[0] \n",
    "        return out \n",
    " \n",
    "    def forward(self, x): \n",
    "        flatten = self._extract_features(x) \n",
    "        z = self.bn(flatten) \n",
    "        z = self.dropout(z) \n",
    "        logits = self.fc_head(z) \n",
    "        e = F.relu(self.hidden1(z)) \n",
    "        e = F.relu(self.hidden2(e)) \n",
    "        layer_dict = { \n",
    "            'base_model_output': self.mixed_7c_output, \n",
    "            'hidden_layer1': F.relu(self.hidden1(self.bn(flatten))), \n",
    "            'hidden_layer2': e, \n",
    "            'flatten': flatten, \n",
    "            'final_logits': logits \n",
    "        } \n",
    "        return logits, e, layer_dict \n",
    " \n",
    "# ------------------------------- \n",
    "# 4. PPDRQ & Losses \n",
    "# ------------------------------- \n",
    " \n",
    "def compute_ppdrq_from_logits(logits: torch.Tensor, num_classes: int) -> torch.Tensor: \n",
    "    probs = torch.softmax(logits, dim=1) \n",
    "    if num_classes == 3: \n",
    "        p1, p2, p3 = probs[:, 0], probs[:, 1], probs[:, 2] \n",
    "        d12 = (p1 - p2).abs(); d13 = (p1 - p3).abs(); d23 = (p2 - p3).abs() \n",
    "        raw = (d12 + d13 + d23) / 3.0 \n",
    "        pp = (3/2) * raw \n",
    "        return torch.clamp(pp, 0, 1) \n",
    "    # general case \n",
    "    diffs = [] \n",
    "    for i in range(num_classes): \n",
    "        for j in range(i+1, num_classes): \n",
    "            diffs.append((probs[:, i] - probs[:, j]).abs()) \n",
    "    sumdiff = torch.stack(diffs, dim=1).sum(dim=1) \n",
    "    pp = sumdiff / (num_classes - 1) \n",
    "    return torch.clamp(pp, 0, 1) \n",
    " \n",
    "class CombinedLoss(nn.Module): \n",
    "    def __init__(self, num_classes, lambda_triplet=Config.lambda_triplet, margin=Config.triplet_margin, \n",
    "                 epsilon_p=Config.epsilon_p, epsilon_n=Config.epsilon_n, smoothing=Config.label_smoothing): \n",
    "        super().__init__() \n",
    "        self.num_classes = num_classes \n",
    "        self.lambda_triplet = lambda_triplet \n",
    "        self.epsilon_p = epsilon_p \n",
    "        self.epsilon_n = epsilon_n \n",
    "        self.triplet = nn.TripletMarginLoss(margin=margin, p=2) \n",
    "        self.smoothing = smoothing \n",
    " \n",
    "    def forward(self, logits, embeddings, labels, \n",
    "                all_feats=None, all_labels=None, all_pp=None): \n",
    "        pp = compute_ppdrq_from_logits(logits, self.num_classes) \n",
    "        per_sample_ce = F.cross_entropy(logits, labels, reduction='none', label_smoothing=self.smoothing) \n",
    "        weights = 1 + (1 - pp) \n",
    "        ce_loss = torch.mean(per_sample_ce * weights) \n",
    " \n",
    "        triplet_loss = torch.tensor(0.0, device=logits.device) \n",
    "        if all_feats is not None and len(all_feats) > 0: \n",
    "            allF = torch.cat(all_feats, dim=0).to(embeddings.device) \n",
    "            allY = torch.cat(all_labels, dim=0).to(embeddings.device) \n",
    "            allP = torch.cat(all_pp, dim=0).to(embeddings.device) \n",
    "            count = 0 \n",
    "            for i in range(labels.size(0)): \n",
    "                a = embeddings[i] \n",
    "                y = labels[i] \n",
    "                ppa = pp[i] \n",
    "                pos_idx = torch.where((allY == y) & ((allP - ppa).abs() < Config.epsilon_p))[0] \n",
    "                neg_idx = torch.where((allY != y) & ((allP - ppa).abs() >= Config.epsilon_n))[0] \n",
    "                if pos_idx.numel() > 1 and neg_idx.numel() > 0: \n",
    "                    p = allF[random.choice(pos_idx.tolist())] \n",
    "                    n = allF[random.choice(neg_idx.tolist())] \n",
    "                    triplet_loss = triplet_loss + self.triplet(a.unsqueeze(0), p.unsqueeze(0), n.unsqueeze(0)) \n",
    "                    count += 1 \n",
    "            if count > 0: \n",
    "                triplet_loss = triplet_loss / count \n",
    "        total = ce_loss + self.lambda_triplet * triplet_loss \n",
    "        return total, ce_loss, triplet_loss, pp \n",
    " \n",
    "# ------------------------------- \n",
    "# 5. Mixup/CutMix utils \n",
    "# ------------------------------- \n",
    " \n",
    "def rand_bbox(W, H, lam): \n",
    "    cut_rat = math.sqrt(1. - lam) \n",
    "    cut_w = int(W * cut_rat) \n",
    "    cut_h = int(H * cut_rat) \n",
    "    cx = np.random.randint(W) \n",
    "    cy = np.random.randint(H) \n",
    "    x1 = np.clip(cx - cut_w // 2, 0, W) \n",
    "    y1 = np.clip(cy - cut_h // 2, 0, H) \n",
    "    x2 = np.clip(cx + cut_w // 2, 0, W) \n",
    "    y2 = np.clip(cy + cut_h // 2, 0, H) \n",
    "    return x1, y1, x2, y2 \n",
    " \n",
    " \n",
    "def apply_mixup_cutmix(x, y): \n",
    "    if not Config.use_mixup_cutmix: \n",
    "        return x, y, None \n",
    "    r = random.random() \n",
    "    if r < 0.5: \n",
    "        # mixup \n",
    "        lam = np.random.beta(Config.mixup_alpha, Config.mixup_alpha) \n",
    "        idx = torch.randperm(x.size(0)).to(x.device) \n",
    "        mixed_x = lam * x + (1 - lam) * x[idx] \n",
    "        y_a, y_b = y, y[idx] \n",
    "        return mixed_x, (y_a, y_b, lam), 'mixup' \n",
    "    else: \n",
    "        # cutmix \n",
    "        lam = np.random.beta(Config.cutmix_alpha, Config.cutmix_alpha) \n",
    "        idx = torch.randperm(x.size(0)).to(x.device) \n",
    "        x1, y1, x2, y2 = rand_bbox(x.size(3), x.size(2), lam) \n",
    "        x_mix = x.clone() \n",
    "        x_mix[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2] \n",
    "        lam = 1 - ((x2 - x1) * (y2 - y1) / (x.size(-1) * x.size(-2))) \n",
    "        y_a, y_b = y, y[idx] \n",
    "        return x_mix, (y_a, y_b, lam), 'cutmix' \n",
    " \n",
    " \n",
    "def mix_criterion(logits, target_tuple): \n",
    "    y_a, y_b, lam = target_tuple \n",
    "    loss_a = F.cross_entropy(logits, y_a, label_smoothing=Config.label_smoothing) \n",
    "    loss_b = F.cross_entropy(logits, y_b, label_smoothing=Config.label_smoothing) \n",
    "    return lam * loss_a + (1 - lam) * loss_b \n",
    " \n",
    "# ------------------------------- \n",
    "# 6. Train/Eval \n",
    "# ------------------------------- \n",
    " \n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, model_name, num_epochs=Config.epochs): \n",
    "    best_val = float('inf') \n",
    "    epochs_no_improve = 0 \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'train_ppdrq': [], 'val_ppdrq': [], 'epochs_trained': 0} \n",
    "    start = time.time() \n",
    " \n",
    "    print(f\"\\n--- Training {model_name} ---\") \n",
    "    for epoch in range(num_epochs): \n",
    "        model.train() \n",
    "        t_loss = 0.0; t_correct = 0; t_total = 0; t_pp = 0.0 \n",
    " \n",
    "        # Pre-collect for triplet \n",
    "        allF, allY, allP = [], [], [] \n",
    "        with torch.no_grad(): \n",
    "            for xb, yb, _ in train_loader: \n",
    "                xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device) \n",
    "                logits, emb, _ = model(xb) \n",
    "                pp = compute_ppdrq_from_logits(logits, Config.num_classes) \n",
    "                allF.append(emb.detach().cpu()); allY.append(yb.detach().cpu()); allP.append(pp.detach().cpu()) \n",
    "        allF = torch.cat(allF, 0) if allF else torch.tensor([]) \n",
    "        allY = torch.cat(allY, 0) if allY else torch.tensor([]) \n",
    "        allP = torch.cat(allP, 0) if allP else torch.tensor([]) \n",
    " \n",
    "        for xb, yb, _ in train_loader: \n",
    "            xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device) \n",
    "            # Mixup/CutMix \n",
    "            xb_m, yb_m, aug = apply_mixup_cutmix(xb, yb) \n",
    " \n",
    "            optimizer.zero_grad(set_to_none=True) \n",
    "            logits, emb, _ = model(xb_m) \n",
    " \n",
    "            if aug is None: \n",
    "                total, ce, trip, pp = criterion(logits, emb, yb, [allF], [allY], [allP]) \n",
    "            else: \n",
    "                # mix criterion for CE part; keep triplet off for stability when mixing \n",
    "                ce_mix = mix_criterion(logits, yb_m) \n",
    "                pp = compute_ppdrq_from_logits(logits, Config.num_classes) \n",
    "                total = ce_mix \n",
    "                ce, trip = ce_mix, torch.tensor(0.0, device=logits.device) \n",
    " \n",
    "            total.backward() \n",
    "            if Config.grad_clip is not None: \n",
    "                nn.utils.clip_grad_norm_(model.parameters(), Config.grad_clip) \n",
    "            optimizer.step() \n",
    " \n",
    "            with torch.no_grad(): \n",
    "                probs = torch.softmax(logits, dim=1) \n",
    "                pred = probs.argmax(1) \n",
    "                if aug is None: \n",
    "                    t_correct += (pred == yb).sum().item() \n",
    "                    t_total += yb.size(0) \n",
    "                else: \n",
    "                    # approximate accuracy against y_a \n",
    "                    y_a, _, lam = yb_m \n",
    "                    t_correct += (pred == y_a).sum().item() * lam \n",
    "                    t_total += y_a.size(0) \n",
    "                t_loss += total.item() * xb.size(0) \n",
    "                t_pp += pp.sum().item() \n",
    " \n",
    "        tr_loss = t_loss / max(t_total, 1) \n",
    "        tr_acc = t_correct / max(t_total, 1) \n",
    "        tr_pp = t_pp / max(t_total, 1) \n",
    " \n",
    "        # Validation \n",
    "        model.eval() \n",
    "        v_loss=0.0; v_cor=0; v_tot=0; v_pp=0.0 \n",
    "        with torch.no_grad(): \n",
    "            for xb, yb, _ in val_loader: \n",
    "                xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device) \n",
    "                logits, emb, _ = model(xb) \n",
    "                total, ce, trip, pp = criterion(logits, emb, yb) \n",
    "                probs = torch.softmax(logits, dim=1) \n",
    "                pred = probs.argmax(1) \n",
    "                v_cor += (pred == yb).sum().item() \n",
    "                v_tot += yb.size(0) \n",
    "                v_loss += total.item() * xb.size(0) \n",
    "                v_pp += pp.sum().item() \n",
    "        va_loss = v_loss / max(v_tot, 1) \n",
    "        va_acc  = v_cor / max(v_tot, 1) \n",
    "        va_pp   = v_pp / max(v_tot, 1) \n",
    " \n",
    "        history['train_loss'].append(tr_loss) \n",
    "        history['val_loss'].append(va_loss) \n",
    "        history['train_acc'].append(tr_acc) \n",
    "        history['val_acc'].append(va_acc) \n",
    "        history['train_ppdrq'].append(tr_pp) \n",
    "        history['val_ppdrq'].append(va_pp) \n",
    "        history['epochs_trained'] = epoch + 1 \n",
    " \n",
    "        if (epoch+1) == 1 or (epoch+1) % 5 == 0 or (epoch+1) == num_epochs: \n",
    "            print(f\"Epoch {epoch+1:03d}/{num_epochs} | Train: loss {tr_loss:.4f} acc {tr_acc:.4f} pp {tr_pp:.3f} | Val: loss {va_loss:.4f} acc {va_acc:.4f} pp {va_pp:.3f}\") \n",
    " \n",
    "        scheduler.step(va_loss) \n",
    "        if va_loss < best_val: \n",
    "            best_val = va_loss \n",
    "            epochs_no_improve = 0 \n",
    "            torch.save(model.state_dict(), os.path.join(Config.output_dir, f\"{model_name}_best.pth\")) \n",
    "        else: \n",
    "            epochs_no_improve += 1 \n",
    "            if epochs_no_improve >= Config.patience_early_stopping: \n",
    "                print(f\"Early stopping at epoch {epoch+1}\") \n",
    "                break \n",
    " \n",
    "    dur = time.time() - start \n",
    "    print(f\"Training time for {model_name}: {dur:.1f}s\") \n",
    "    return history, dur \n",
    " \n",
    " \n",
    "def calculate_calibration_metrics(probabilities: torch.Tensor, labels: torch.Tensor, num_bins=10): \n",
    "    bins = torch.linspace(0, 1, num_bins + 1) \n",
    "    conf, pred = probabilities.max(1) \n",
    "    acc = (pred == labels).float() \n",
    "    ece = 0.0 \n",
    "    for i in range(num_bins): \n",
    "        in_bin = (conf > bins[i]) & (conf <= bins[i+1]) \n",
    "        if in_bin.any(): \n",
    "            ece += torch.abs(acc[in_bin].mean() - conf[in_bin].mean()) * in_bin.float().mean() \n",
    "    one_hot = F.one_hot(labels, num_classes=Config.num_classes).float() \n",
    "    brier = torch.mean(torch.sum((probabilities - one_hot) ** 2, dim=1)) \n",
    "    return ece.item(), brier.item() \n",
    " \n",
    " \n",
    "def plot_reliability_diagram(conf, correct, ece, model_name, num_bins=10): \n",
    "    bins = np.linspace(0, 1, num_bins + 1) \n",
    "    mids = (bins[:-1] + bins[1:]) / 2 \n",
    "    bin_acc = [] \n",
    "    counts = [] \n",
    "    for i in range(num_bins): \n",
    "        m = (conf >= bins[i]) & (conf <= bins[i+1]) \n",
    "        if m.sum() > 0: \n",
    "            bin_acc.append(correct[m].float().mean().item()) \n",
    "            counts.append(int(m.sum())) \n",
    "        else: \n",
    "            bin_acc.append(0.0); counts.append(0) \n",
    "    plt.figure(figsize=(6,6)) \n",
    "    plt.plot([0,1],[0,1],'k:') \n",
    "    plt.bar(mids, bin_acc, width=1/num_bins*0.9, alpha=0.7, edgecolor='black') \n",
    "    for i, c in enumerate(counts): \n",
    "        if c>0: \n",
    "            plt.text(mids[i], bin_acc[i]+0.02, str(c), ha='center', fontsize=8) \n",
    "    plt.title(f\"Reliability: {model_name} (ECE={ece:.3f})\") \n",
    "    plt.xlabel('Confidence'); plt.ylabel('Accuracy'); plt.ylim(0,1) \n",
    "    plt.grid(True) \n",
    "    plt.savefig(os.path.join(Config.output_dir, f\"{model_name}_reliability.png\")) \n",
    "    plt.close() \n",
    " \n",
    " \n",
    "def evaluate_model(model, loader, model_name): \n",
    "    model.eval() \n",
    "    all_prob=[]; all_y=[] \n",
    "    t0=time.time() \n",
    "    if len(loader.dataset)==0: \n",
    "        print(f\"[WARN] {model_name} loader empty.\") \n",
    "        return {k:0.0 for k in ['accuracy','precision','recall','f1_score','mean_ppdrq','ece','brier_score','inference_time']} \n",
    "    with torch.no_grad(): \n",
    "        for xb, yb, _ in tqdm(loader, desc=f\"Evaluating {model_name}\"): \n",
    "            xb=xb.to(Config.device); yb=torch.as_tensor(yb, device=Config.device) \n",
    "            logits, _, _ = model(xb) \n",
    "            prob = torch.softmax(logits, dim=1) \n",
    "            all_prob.append(prob.cpu()); all_y.append(yb.cpu()) \n",
    "    t1=time.time() \n",
    "    prob=torch.cat(all_prob,0) \n",
    "    y=torch.cat(all_y,0) \n",
    "    pred=prob.argmax(1) \n",
    "    acc=accuracy_score(y.numpy(), pred.numpy()) \n",
    "    prec=precision_score(y.numpy(), pred.numpy(), average='macro', zero_division=0) \n",
    "    rec=recall_score(y.numpy(), pred.numpy(), average='macro', zero_division=0) \n",
    "    f1=f1_score(y.numpy(), pred.numpy(), average='macro', zero_division=0) \n",
    "    # PPDRQ computed on logits; use log(prob) as surrogate logits \n",
    "    pp = compute_ppdrq_from_logits(torch.log(prob+1e-8), Config.num_classes) \n",
    "    ece,brier=calculate_calibration_metrics(prob, y) \n",
    "    plot_reliability_diagram(prob.max(1)[0], (pred==y), ece, model_name) \n",
    "    print(f\"\\n[{model_name}] Acc {acc:.4f} Prec {prec:.4f} Rec {rec:.4f} F1 {f1:.4f} PPDRQ {pp.mean().item():.4f} ECE {ece:.4f} Brier {brier:.4f}\") \n",
    "    return { \n",
    "        'accuracy':acc,'precision':prec,'recall':rec,'f1_score':f1, \n",
    "        'mean_ppdrq':pp.mean().item(),'ece':ece,'brier_score':brier, \n",
    "        'inference_time':t1-t0 \n",
    "    } \n",
    " \n",
    "# ------------------------------- \n",
    "# 7. Layer-wise PPDRQ \n",
    "# ------------------------------- \n",
    " \n",
    "def get_and_plot_layerwise_ppdrq(model, loader, model_name, title_suffix=\"\"): \n",
    "    model.eval() \n",
    "    collect = {k:[] for k in ['base_model_output','hidden_layer1','hidden_layer2','flatten','final_logits']} \n",
    "    if len(loader.dataset)==0: \n",
    "        print(f\"[WARN] No data for layer-wise PPDRQ.\") \n",
    "        return \n",
    "    with torch.no_grad(): \n",
    "        for xb, _, _ in loader: \n",
    "            xb=xb.to(Config.device) \n",
    "            logits, _, d = model(xb) \n",
    "            for k,v in d.items(): \n",
    "                if k=='base_model_output': \n",
    "                    collect[k].append(v.mean(dim=(2,3)).cpu()) \n",
    "                else: \n",
    "                    collect[k].append(v.cpu()) \n",
    "    temp_base = nn.Linear(2048, Config.num_classes) \n",
    "    temp_h1   = nn.Linear(1024, Config.num_classes) \n",
    "    temp_h2   = nn.Linear(512, Config.num_classes) \n",
    "    temp_flat = nn.Linear(2048, Config.num_classes) \n",
    "    mean_vals={} \n",
    "    for k, arr in collect.items(): \n",
    "        if not arr: mean_vals[k]=0.0; continue \n",
    "        X = torch.cat(arr,0) \n",
    "        if k=='base_model_output': \n",
    "            l = temp_base(X) \n",
    "        elif k=='hidden_layer1': \n",
    "            l = temp_h1(X) \n",
    "        elif k=='hidden_layer2': \n",
    "            l = temp_h2(X) \n",
    "        elif k=='flatten': \n",
    "            l = temp_flat(X) \n",
    "        else: # final_logits \n",
    "            l = X \n",
    "        pp = compute_ppdrq_from_logits(l, Config.num_classes) \n",
    "        mean_vals[k]=pp.mean().item() \n",
    "    mmax=max(mean_vals.values()) if mean_vals else 1.0 \n",
    "    norm={k:(v/mmax if mmax>0 else 0.0) for k,v in mean_vals.items()} \n",
    "    layers=list(norm.keys()); vals=[norm[k] for k in layers] \n",
    "    plt.figure(figsize=(9,5)) \n",
    "    plt.bar(layers, vals) \n",
    "    for i,v in enumerate(vals): \n",
    "        plt.text(i, v+0.02, f\"{v:.2f}\", ha='center') \n",
    "    plt.ylim(0,1); plt.ylabel('Normalized Mean PPDRQ'); plt.title(f\"Layer PPDRQ {title_suffix}\") \n",
    "    plt.savefig(os.path.join(Config.output_dir, f\"{model_name}_layer_ppdrq.png\")) \n",
    "    plt.close() \n",
    " \n",
    "# ------------------------------- \n",
    "# 8. MC-Dropout & Ensemble \n",
    "# ------------------------------- \n",
    " \n",
    "def _set_dropout_mode_only(model: nn.Module, training: bool = True): \n",
    "    \"\"\"Toggle ONLY Dropout layers' mode without touching BN or others.\"\"\" \n",
    "    for m in model.modules(): \n",
    "        if isinstance(m, (nn.Dropout, nn.Dropout2d, nn.Dropout3d, nn.AlphaDropout)): \n",
    "            m.train(training)  # enable stochastic masks \n",
    "        # do NOT touch BatchNorm or the rest \n",
    " \n",
    "def mc_dropout_predict(model, loader, num_passes=Config.mc_dropout_passes): \n",
    "    # --- FIXED STOCHASTIC INFERENCE PROTOCOL --- \n",
    "    # Keep global eval() to freeze BatchNorm running stats; \n",
    "    # enable randomness ONLY in Dropout layers. \n",
    "    model.eval() \n",
    "    _set_dropout_mode_only(model, True) \n",
    " \n",
    "    all_prob=[]; all_y=[]; t0=time.time() \n",
    "    with torch.no_grad(): \n",
    "        for xb, yb, _ in tqdm(loader, desc=f\"MC-Dropout {num_passes} passes\"): \n",
    "            xb=xb.to(Config.device); yb=torch.as_tensor(yb, device=Config.device) \n",
    "            batch = [] \n",
    "            for _ in range(num_passes): \n",
    "                logits, _, _ = model(xb) \n",
    "                batch.append(torch.softmax(logits, dim=1).unsqueeze(0)) \n",
    "            mean_prob = torch.cat(batch,0).mean(0) \n",
    "            all_prob.append(mean_prob.cpu()); all_y.append(yb.cpu()) \n",
    "    t1=time.time() \n",
    " \n",
    "    # restore Dropout to eval mode \n",
    "    _set_dropout_mode_only(model, False) \n",
    " \n",
    "    prob=torch.cat(all_prob,0); y=torch.cat(all_y,0) \n",
    "    pred=prob.argmax(1) \n",
    "    acc=accuracy_score(y.numpy(), pred.numpy()) \n",
    "    prec=precision_score(y.numpy(), pred.numpy(), average='macro', zero_division=0) \n",
    "    rec=recall_score(y.numpy(), pred.numpy(), average='macro', zero_division=0) \n",
    "    f1=f1_score(y.numpy(), pred.numpy(), average='macro', zero_division=0) \n",
    "    ece,brier=calculate_calibration_metrics(prob, y) \n",
    "    plot_reliability_diagram(prob.max(1)[0], (pred==y), ece, 'MC-Dropout') \n",
    "    pp = compute_ppdrq_from_logits(torch.log(prob+1e-8), Config.num_classes) \n",
    "    print(f\"\\n[MC-Dropout] Acc {acc:.4f} F1 {f1:.4f} PPDRQ {pp.mean().item():.4f} ECE {ece:.4f}\") \n",
    "    return {'accuracy':acc,'precision':prec,'recall':rec,'f1_score':f1, \n",
    "            'mean_ppdrq':pp.mean().item(),'ece':ece,'brier_score':brier, \n",
    "            'inference_time':t1-t0} \n",
    " \n",
    " \n",
    "def train_ensemble_member(i, train_loader, val_loader): \n",
    "    print(f\"\\n--- Training Ensemble Member {i+1} ---\") \n",
    "    m = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device) \n",
    "    crit = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=0.0)  # CE only with smoothing \n",
    "    opt = optim.AdamW(filter(lambda p: p.requires_grad, m.parameters()), lr=Config.learning_rate, weight_decay=Config.weight_decay) \n",
    "    sch = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.2, patience=Config.patience_lr_scheduler) \n",
    "    hist, t = train_model(m, train_loader, val_loader, crit, opt, sch, f\"Ensemble_{i+1}\", num_epochs=max(10, Config.epochs//Config.num_ensemble_models)) \n",
    "    torch.save(m.state_dict(), os.path.join(Config.output_dir, f\"ensemble_member_{i+1}.pth\")) \n",
    "    return m, t, hist \n",
    " \n",
    " \n",
    "def evaluate_deep_ensemble(models_list: List[nn.Module], loader): \n",
    "    all_prob=[]; all_y=[]; t0=time.time() \n",
    "    with torch.no_grad(): \n",
    "        for xb, yb, _ in tqdm(loader, desc=\"Evaluating Ensemble\"): \n",
    "            xb=xb.to(Config.device) \n",
    "            mem_probs=[] \n",
    "            for m in models_list: \n",
    "                m.eval() \n",
    "                logits, _, _ = m(xb) \n",
    "                mem_probs.append(torch.softmax(logits, dim=1).unsqueeze(0)) \n",
    "            mean_prob=torch.cat(mem_probs,0).mean(0) \n",
    "            all_prob.append(mean_prob.cpu()); all_y.append(torch.as_tensor(yb).cpu()) \n",
    "    t1=time.time() \n",
    "    prob=torch.cat(all_prob,0); y=torch.cat(all_y,0) \n",
    "    pred=prob.argmax(1) \n",
    "    acc=accuracy_score(y.numpy(), pred.numpy()) \n",
    "    prec=precision_score(y.numpy(), pred.numpy(), average='macro', zero_division=0) \n",
    "    rec=recall_score(y.numpy(), pred.numpy(), average='macro', zero_division=0) \n",
    "    f1=f1_score(y.numpy(), pred.numpy(), average='macro', zero_division=0) \n",
    "    ece,brier=calculate_calibration_metrics(prob, y) \n",
    "    plot_reliability_diagram(prob.max(1)[0], (pred==y), ece, 'Deep-Ensemble') \n",
    "    pp = compute_ppdrq_from_logits(torch.log(prob+1e-8), Config.num_classes) \n",
    "    print(f\"\\n[Ensemble] Acc {acc:.4f} F1 {f1:.4f} PPDRQ {pp.mean().item():.4f} ECE {ece:.4f}\") \n",
    "    return {'accuracy':acc,'precision':prec,'recall':rec,'f1_score':f1, \n",
    "            'mean_ppdrq':pp.mean().item(),'ece':ece,'brier_score':brier, \n",
    "            'inference_time':t1-t0} \n",
    " \n",
    "# ------------------------------- \n",
    "# 9. Paper-Ready Reporting Utilities \n",
    "# ------------------------------- \n",
    " \n",
    "def save_results_table(results: Dict[str, Dict], times: Dict[str, float], path_csv: str): \n",
    "    rows = [] \n",
    "    for name, m in results.items(): \n",
    "        train_t = times.get(name, times.get(f\"{name}_Train\", np.nan)) \n",
    "        infer_t = m.get('inference_time', times.get(f\"{name}_Infer\", np.nan)) \n",
    "        rows.append({ \n",
    "            'Model': name, \n",
    "            'Accuracy': m['accuracy'], 'Precision': m['precision'], 'Recall': m['recall'], 'F1': m['f1_score'], \n",
    "            'Mean_PPDRQ': m['mean_ppdrq'], 'ECE': m['ece'], 'Brier': m['brier_score'], \n",
    "            'Train_Time_s': train_t, 'Infer_Time_s': infer_t \n",
    "        }) \n",
    "    df = pd.DataFrame(rows) \n",
    "    csv_path = os.path.join(Config.output_dir, path_csv) \n",
    "    df.to_csv(csv_path, index=False) \n",
    "    print(f\"Saved results table -> {csv_path}\") \n",
    "    return df \n",
    " \n",
    " \n",
    "def plot_metric_bars(df: pd.DataFrame, metric: str, fname: str): \n",
    "    plt.figure(figsize=(8,4)) \n",
    "    order = df.sort_values(metric, ascending=False)['Model'] \n",
    "    plt.bar(order, df.set_index('Model').loc[order, metric]) \n",
    "    plt.xticks(rotation=25, ha='right') \n",
    "    plt.ylabel(metric) \n",
    "    plt.title(f\"Model comparison: {metric}\") \n",
    "    out = os.path.join(Config.output_dir, fname) \n",
    "    plt.tight_layout(); plt.savefig(out); plt.close() \n",
    "    print(f\"Saved {metric} bar plot -> {out}\") \n",
    " \n",
    " \n",
    "def plot_cost_performance(df: pd.DataFrame, perf_metric: str='Accuracy', cost_metric: str='Infer_Time_s', fname: str='cost_perf.png'): \n",
    "    plt.figure(figsize=(5,5)) \n",
    "    x = df[cost_metric].values; y = df[perf_metric].values \n",
    "    for i, row in df.iterrows(): \n",
    "        plt.scatter(row[cost_metric], row[perf_metric]) \n",
    "        plt.text(row[cost_metric], row[perf_metric]+1e-3, row['Model'], fontsize=8) \n",
    "    plt.xlabel(cost_metric); plt.ylabel(perf_metric) \n",
    "    plt.title(f\"Performance-Cost Trade-off ({perf_metric} vs {cost_metric})\") \n",
    "    out = os.path.join(Config.output_dir, fname) \n",
    "    plt.grid(True); plt.tight_layout(); plt.savefig(out); plt.close() \n",
    "    print(f\"Saved cost-performance plot -> {out}\") \n",
    " \n",
    " \n",
    "def lambda_sensitivity_sweep(lambdas=(0.0, 0.01, 0.1, 0.3, 1.0)): \n",
    "    \"\"\"Re-train small runs with different λ (triplet weight) and plot Accuracy/ECE vs λ. \n",
    "    Warning: This re-trains models. Reduce epochs or subset data for quick runs. \n",
    "    \"\"\" \n",
    "    acc_list=[]; ece_list=[] \n",
    "    for lam in lambdas: \n",
    "        print(f\"\\n[λ-sweep] Training with λ={lam}\") \n",
    "        model = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device) \n",
    "        crit = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=lam) \n",
    "        opt = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=Config.learning_rate, weight_decay=Config.weight_decay) \n",
    "        sch = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.2, patience=Config.patience_lr_scheduler) \n",
    "        hist, _ = train_model(model, train_loader, val_loader, crit, opt, sch, f'Lambda_{lam}', num_epochs=max(5, Config.epochs//6)) \n",
    "        res = evaluate_model(model, test_loader, f'Lambda_{lam}') \n",
    "        acc_list.append(res['accuracy']); ece_list.append(res['ece']) \n",
    "    plt.figure(figsize=(6,4)) \n",
    "    plt.plot(list(lambdas), acc_list, marker='o', label='Accuracy') \n",
    "    plt.plot(list(lambdas), ece_list, marker='o', label='ECE') \n",
    "    plt.xlabel('λ (triplet weight)'); plt.legend(); plt.grid(True) \n",
    "    out = os.path.join(Config.output_dir, 'lambda_sensitivity.png') \n",
    "    plt.tight_layout(); plt.savefig(out); plt.close() \n",
    "    print(f\"Saved λ sensitivity plot -> {out}\") \n",
    " \n",
    "# ------------------------------- \n",
    "# 9b. EXTRA visualizations (added; nothing replaced) \n",
    "# ------------------------------- \n",
    "CLASS_NAMES = ['AD','MCI','NC'] \n",
    " \n",
    "def collect_predictions(model, loader): \n",
    "    y_true=[]; y_pred=[]; conf=[]; probs=[] \n",
    "    model.eval() \n",
    "    with torch.no_grad(): \n",
    "        for xb, yb, _ in loader: \n",
    "            xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device) \n",
    "            logits, _, _ = model(xb) \n",
    "            p = torch.softmax(logits, dim=1) \n",
    "            pr = p.argmax(1) \n",
    "            y_true.append(yb.cpu()); y_pred.append(pr.cpu()); conf.append(p.max(1)[0].cpu()); probs.append(p.cpu()) \n",
    "    return torch.cat(y_true).numpy(), torch.cat(y_pred).numpy(), torch.cat(conf).numpy(), torch.cat(probs,0).numpy() \n",
    " \n",
    "def plot_confusion(y_true, y_pred, classes, title, fname, normalize=True): \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes)))) \n",
    "    if normalize: \n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True).clip(min=1) \n",
    "    plt.figure(figsize=(5,4)) \n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues') \n",
    "    plt.title(title); plt.colorbar() \n",
    "    tick_marks = np.arange(len(classes)) \n",
    "    plt.xticks(tick_marks, classes, rotation=45); plt.yticks(tick_marks, classes) \n",
    "    thresh = cm.max() / 2. \n",
    "    for i in range(cm.shape[0]): \n",
    "        for j in range(cm.shape[1]): \n",
    "            plt.text(j, i, f\"{cm[i, j]:.2f}\" if normalize else int(cm[i, j]), \n",
    "                     ha=\"center\", va=\"center\", \n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\") \n",
    "    plt.ylabel('True'); plt.xlabel('Predicted'); plt.tight_layout() \n",
    "    out = os.path.join(Config.output_dir, fname) \n",
    "    plt.savefig(out); plt.close(); print(f\"Saved confusion matrix -> {out}\") \n",
    " \n",
    "def export_per_class_report(y_true, y_pred, classes, fname_csv): \n",
    "    rep = classification_report(y_true, y_pred, target_names=classes, output_dict=True, zero_division=0) \n",
    "    df = pd.DataFrame(rep).transpose() \n",
    "    out = os.path.join(Config.output_dir, fname_csv) \n",
    "    df.to_csv(out) \n",
    "    print(f\"Saved per-class report -> {out}\") \n",
    " \n",
    "def reliability_grid(models_dict, loader, fname='reliability_grid.png', num_bins=10): \n",
    "    cols = len(models_dict); plt.figure(figsize=(5*cols,5)) \n",
    "    for idx,(name, model) in enumerate(models_dict.items(), start=1): \n",
    "        model.eval(); all_prob=[]; all_y=[] \n",
    "        with torch.no_grad(): \n",
    "            for xb, yb, _ in loader: \n",
    "                xb=xb.to(Config.device); yb=torch.as_tensor(yb, device=Config.device) \n",
    "                logits, _, _ = model(xb); pr = torch.softmax(logits, dim=1) \n",
    "                all_prob.append(pr.cpu()); all_y.append(yb.cpu()) \n",
    "        prob=torch.cat(all_prob,0); y=torch.cat(all_y,0); pred=prob.argmax(1) \n",
    "        conf=prob.max(1)[0]; correct=(pred==y) \n",
    "        # compute ECE for title \n",
    "        ece,_ = calculate_calibration_metrics(prob, y) \n",
    "        bins = np.linspace(0,1,num_bins+1); mids=(bins[:-1]+bins[1:])/2 \n",
    "        bin_acc=[] \n",
    "        for i in range(num_bins): \n",
    "            m = (conf>=bins[i]) & (conf<=bins[i+1]) \n",
    "            bin_acc.append(correct[m].float().mean().item() if m.sum()>0 else 0.0) \n",
    "        ax = plt.subplot(1, cols, idx) \n",
    "        ax.plot([0,1],[0,1],'k:'); ax.bar(mids, bin_acc, width=1/num_bins*0.9, alpha=0.7, edgecolor='black') \n",
    "        ax.set_title(f\"{name} (ECE={ece:.3f})\"); ax.set_xlabel('Confidence'); ax.set_ylabel('Accuracy'); ax.set_ylim(0,1); ax.grid(True) \n",
    "    out = os.path.join(Config.output_dir, fname) \n",
    "    plt.tight_layout(); plt.savefig(out); plt.close(); print(f\"Saved reliability grid -> {out}\") \n",
    " \n",
    "# ------------------------------- \n",
    "# 10. Main \n",
    "# ------------------------------- \n",
    "if __name__ == '__main__': \n",
    "    results: Dict[str, Dict] = {} \n",
    "    times: Dict[str, float] = {} \n",
    " \n",
    "    # Baseline (with label smoothing + anti-overfitting) \n",
    "    baseline = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device) \n",
    "    crit_base = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=0.0)  # CE only \n",
    "    opt_base = optim.AdamW(filter(lambda p: p.requires_grad, baseline.parameters()), lr=Config.learning_rate, weight_decay=Config.weight_decay) \n",
    "    sch_base = optim.lr_scheduler.ReduceLROnPlateau(opt_base, mode='min', factor=0.2, patience=Config.patience_lr_scheduler) \n",
    "    hist_b, t_b = train_model(baseline, train_loader, val_loader, crit_base, opt_base, sch_base, 'Baseline') \n",
    "    res_b = evaluate_model(baseline, test_loader, 'Baseline') \n",
    "    results['Baseline'] = res_b; times['Baseline'] = t_b \n",
    " \n",
    "    # PPDRQ-weighted CE only \n",
    "    class PPDRQWeightedCE(nn.Module): \n",
    "        def __init__(self, num_classes): \n",
    "            super().__init__(); self.num_classes=num_classes \n",
    "        def forward(self, logits, _emb, labels, **kwargs): \n",
    "            pp = compute_ppdrq_from_logits(logits, self.num_classes) \n",
    "            per_ce = F.cross_entropy(logits, labels, reduction='none', label_smoothing=Config.label_smoothing) \n",
    "            w = 1 + (1 - pp) \n",
    "            loss = torch.mean(per_ce * w) \n",
    "            return loss, loss, torch.tensor(0.0, device=logits.device), pp \n",
    " \n",
    "    ppdrq_model = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device) \n",
    "    crit_pp = PPDRQWeightedCE(num_classes=Config.num_classes) \n",
    "    opt_pp = optim.AdamW(filter(lambda p: p.requires_grad, ppdrq_model.parameters()), lr=Config.learning_rate, weight_decay=Config.weight_decay) \n",
    "    sch_pp = optim.lr_scheduler.ReduceLROnPlateau(opt_pp, mode='min', factor=0.2, patience=Config.patience_lr_scheduler) \n",
    "    hist_p, t_p = train_model(ppdrq_model, train_loader, val_loader, crit_pp, opt_pp, sch_pp, 'PPDRQ_CE') \n",
    "    res_p = evaluate_model(ppdrq_model, test_loader, 'PPDRQ_CE') \n",
    "    results['PPDRQ_CE'] = res_p; times['PPDRQ_CE'] = t_p \n",
    " \n",
    "    # Triplet model \n",
    "    triplet_model = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device) \n",
    "    crit_trip = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=Config.lambda_triplet) \n",
    "    opt_trip = optim.AdamW(filter(lambda p: p.requires_grad, triplet_model.parameters()), lr=Config.learning_rate, weight_decay=Config.weight_decay) \n",
    "    sch_trip = optim.lr_scheduler.ReduceLROnPlateau(opt_trip, mode='min', factor=0.2, patience=Config.patience_lr_scheduler) \n",
    "    hist_t, t_t = train_model(triplet_model, train_loader, val_loader, crit_trip, opt_trip, sch_trip, 'Triplet') \n",
    "    res_t = evaluate_model(triplet_model, test_loader, 'Triplet') \n",
    "    results['Triplet'] = res_t; times['Triplet'] = t_t \n",
    " \n",
    "    # MC-Dropout (train normally; eval with stochastic passes) \n",
    "    mc_model = CustomInceptionV3(num_classes=Config.num_classes, dropout_rate=0.6).to(Config.device) \n",
    "    crit_mc = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=0.0) \n",
    "    opt_mc = optim.AdamW(filter(lambda p: p.requires_grad, mc_model.parameters()), lr=Config.learning_rate, weight_decay=Config.weight_decay) \n",
    "    sch_mc = optim.lr_scheduler.ReduceLROnPlateau(opt_mc, mode='min', factor=0.2, patience=Config.patience_lr_scheduler) \n",
    "    hist_m, t_m = train_model(mc_model, train_loader, val_loader, crit_mc, opt_mc, sch_mc, 'MC_Dropout') \n",
    "    res_m = mc_dropout_predict(mc_model, test_loader, num_passes=Config.mc_dropout_passes) \n",
    "    results['MC_Dropout'] = res_m; times['MC_Dropout_Train'] = t_m; times['MC_Dropout_Infer'] = res_m['inference_time'] \n",
    " \n",
    "    # Deep Ensemble (5 members) \n",
    "    members=[]; t_mem=[] \n",
    "    for i in range(Config.num_ensemble_models): \n",
    "        m, t, _ = train_ensemble_member(i, train_loader, val_loader) \n",
    "        members.append(m); t_mem.append(t) \n",
    "    res_e = evaluate_deep_ensemble(members, test_loader) \n",
    "    results['Ensemble'] = res_e; times['Ensemble_Train'] = sum(t_mem); times['Ensemble_Infer'] = res_e['inference_time'] \n",
    " \n",
    "    # Summary to console \n",
    "    print(\"\\n--- Summary ---\") \n",
    "    print(f\"{'Model':<15}{'Acc':>8}{'Prec':>8}{'Rec':>8}{'F1':>8}{'PPDRQ':>9}{'ECE':>8}{'Brier':>8}{'Train(s)':>10}{'Infer(s)':>10}\") \n",
    "    for k, v in results.items(): \n",
    "        tr = times.get(k, times.get(f\"{k}_Train\", 0.0)) \n",
    "        inf = v.get('inference_time', times.get(f\"{k}_Infer\", 0.0)) \n",
    "        print(f\"{k:<15}{v['accuracy']:>8.3f}{v['precision']:>8.3f}{v['recall']:>8.3f}{v['f1_score']:>8.3f}{v['mean_ppdrq']:>9.3f}{v['ece']:>8.3f}{v['brier_score']:>8.3f}{tr:>10.1f}{inf:>10.1f}\") \n",
    " \n",
    "    # Save curves for baseline/PPDRQ/Triplet \n",
    "    for name, hist in [('Baseline', hist_b), ('PPDRQ_CE', hist_p), ('Triplet', hist_t)]: \n",
    "        plt.figure(figsize=(10,4)) \n",
    "        plt.subplot(1,2,1) \n",
    "        plt.plot(hist['train_acc'], label='Train'); plt.plot(hist['val_acc'], label='Val'); plt.title(f'{name} Acc') \n",
    "        plt.legend(); plt.subplot(1,2,2) \n",
    "        plt.plot(hist['train_loss'], label='Train'); plt.plot(hist['val_loss'], label='Val'); plt.title(f'{name} Loss') \n",
    "        plt.legend(); plt.tight_layout() \n",
    "        plt.savefig(os.path.join(Config.output_dir, f\"{name}_curves.png\")) \n",
    "        plt.close() \n",
    " \n",
    "    # Unified table + plots for paper \n",
    "    df = save_results_table(results, times, \"results_all_models.csv\") \n",
    "    plot_metric_bars(df, \"Accuracy\", \"cmp_accuracy.png\") \n",
    "    plot_metric_bars(df, \"F1\", \"cmp_f1.png\") \n",
    "    plot_metric_bars(df, \"ECE\", \"cmp_ece.png\") \n",
    "    plot_metric_bars(df, \"Brier\", \"cmp_brier.png\") \n",
    "    plot_cost_performance(df, perf_metric=\"Accuracy\", cost_metric=\"Infer_Time_s\", fname=\"cost_perf_acc_vs_infer.png\") \n",
    " \n",
    "    # EXTRA visuals: confusion matrices, per-class reports, side-by-side reliability grid (test set) \n",
    "    model_objs = { \n",
    "        'Baseline': baseline, \n",
    "        'PPDRQ_CE': ppdrq_model, \n",
    "        'Triplet': triplet_model, \n",
    "        'MC_Dropout': mc_model \n",
    "    } \n",
    "    class EnsembleWrapper(nn.Module): \n",
    "        def __init__(self, members): \n",
    "            super().__init__(); self.members=members \n",
    "        def forward(self, x): \n",
    "            outs=[] \n",
    "            for m in self.members: \n",
    "                m.eval(); lo,_,_ = m(x); outs.append(lo.unsqueeze(0)) \n",
    "            logits = torch.mean(torch.cat(outs,0), dim=0) \n",
    "            return logits, torch.zeros(x.size(0),512, device=logits.device), {'final_logits': logits} \n",
    "    ens_wrapper = EnsembleWrapper(members) \n",
    "    model_objs['Ensemble'] = ens_wrapper \n",
    " \n",
    "    for name, m in model_objs.items(): \n",
    "        y_t, y_p, confs, prob = collect_predictions(m, test_loader) \n",
    "        plot_confusion(y_t, y_p, CLASS_NAMES, f\"Confusion: {name}\", f\"cm_{name}.png\", normalize=True) \n",
    "        export_per_class_report(y_t, y_p, CLASS_NAMES, f\"per_class_{name}.csv\") \n",
    " \n",
    "    reliability_grid(model_objs, test_loader, fname='reliability_grid_test.png', num_bins=10) \n",
    " \n",
    "    # Unseen (domain shift) evaluation if available \n",
    "    if len(unseen_dataset)>0: \n",
    "        print(\"\\n--- Evaluating on UNSEEN domain ---\") \n",
    "        # Load best weights if present \n",
    "        for tag, model in [('Baseline', baseline), ('PPDRQ_CE', ppdrq_model), ('Triplet', triplet_model), ('MC_Dropout', mc_model)]: \n",
    "            pth = os.path.join(Config.output_dir, f\"{tag}_best.pth\") \n",
    "            if os.path.exists(pth): \n",
    "                model.load_state_dict(torch.load(pth, map_location=Config.device)) \n",
    "        # Evaluate \n",
    "        res_b_u = evaluate_model(baseline, unseen_loader, 'Baseline_Unseen') \n",
    "        res_p_u = evaluate_model(ppdrq_model, unseen_loader, 'PPDRQ_CE_Unseen') \n",
    "        res_t_u = evaluate_model(triplet_model, unseen_loader, 'Triplet_Unseen') \n",
    "        res_m_u = mc_dropout_predict(mc_model, unseen_loader, num_passes=Config.mc_dropout_passes) \n",
    "        if len(members)==Config.num_ensemble_models: \n",
    "            res_e_u = evaluate_deep_ensemble(members, unseen_loader) \n",
    "        print(\"--- Unseen evaluation complete ---\") \n",
    " \n",
    "    print(\"\\nDone. Check ppdrq_results/ for saved plots, CSVs, and weights (including EXTRA figures).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c24db",
   "metadata": {},
   "source": [
    "# PPDRQ With\n",
    "1.DATASET VERIFICATION\n",
    "   - Patient-level split verification completed\n",
    "   - See: patient_split_verification.csv\n",
    "   \n",
    "2. MODEL PERFORMANCE\n",
    "   All models evaluated on test set with comprehensive metrics:\n",
    "   - See: results_all_models.csv\n",
    "   \n",
    "3. STATISTICAL SIGNIFICANCE\n",
    "   Bootstrap confidence intervals and McNemar tests computed:\n",
    "   - See: statistical_significance_results.csv\n",
    "   - Visualizations: *_confidence_intervals.png\n",
    "   \n",
    "4. CALIBRATION ANALYSIS\n",
    "   - Reliability diagrams for all models\n",
    "   - ECE and Brier scores computed\n",
    "   - See: reliability_grid_test.png\n",
    "   \n",
    "5. PER-CLASS PERFORMANCE\n",
    "   - Confusion matrices: cm_*.png\n",
    "   - Detailed reports: per_class_*.csv\n",
    "   \n",
    "6. CLINICAL VALIDATION\n",
    "   - Cases selected for neurologist review\n",
    "   - See: clinical_validation_*.csv\n",
    "   - Template provided for clinical observations\n",
    "   \n",
    "7. DOMAIN SHIFT EVALUATION\n",
    "   - Completed on unseen dataset\n",
    "   - See: results_unseen_domain.csv\n",
    "   - Limitation acknowledged in results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f7bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Config\n",
    "# -------------------------------\n",
    "class Config:\n",
    "    data_dir = 'dataset'              # expected: data/{train,validation,test,unseen}/{AD,MCI,NC}\n",
    "    output_dir = 'result save directory'\n",
    "\n",
    "    image_size = 299\n",
    "    num_classes = 3                # AD / MCI / NC\n",
    "\n",
    "    batch_size = 16                \n",
    "    epochs = 60                    \n",
    "    learning_rate = 3e-4\n",
    "    weight_decay = 5e-4            \n",
    "    patience_lr_scheduler = 3\n",
    "    patience_early_stopping = 10\n",
    "    grad_clip = 1.0\n",
    "\n",
    "    # Triplet & PPDRQ\n",
    "    lambda_triplet = 0.1\n",
    "    triplet_margin = 0.2\n",
    "    epsilon_p = 0.1\n",
    "    epsilon_n = 0.2\n",
    "\n",
    "    # MC-Dropout & Ensemble\n",
    "    mc_dropout_passes = 30\n",
    "    mc_dropout_rate = 0.3          # FIXED: Reasonable dropout rate\n",
    "    num_ensemble_models = 5\n",
    "\n",
    "    # Statistical testing\n",
    "    bootstrap_iterations = 1000\n",
    "    confidence_level = 0.95\n",
    "\n",
    "    # Regularization toggles\n",
    "    use_mixup_cutmix = True\n",
    "    mixup_alpha = 0.2\n",
    "    cutmix_alpha = 0.2\n",
    "    label_smoothing = 0.05\n",
    "\n",
    "    # Data safety\n",
    "    allow_make_dummy = False       \n",
    "\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "os.makedirs(Config.output_dir, exist_ok=True)\n",
    "set_seed(42)\n",
    "print(f\"Using device: {Config.device}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Dataset with Patient ID Support\n",
    "# -------------------------------\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, phase: str, transform=None):\n",
    "        self.data_dir = os.path.join(data_dir, phase)\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {'AD': 0, 'MCI': 1, 'NC': 2}\n",
    "        self.image_paths: List[str] = []\n",
    "        self.labels: List[int] = []\n",
    "        self.patient_ids: List[str] = []  # NEW: Track patient IDs\n",
    "\n",
    "        for cname, idx in self.class_to_idx.items():\n",
    "            cdir = os.path.join(self.data_dir, cname)\n",
    "            if not os.path.exists(cdir):\n",
    "                if Config.allow_make_dummy:\n",
    "                    print(f\"[WARN] Missing {cdir}. Creating dummy images for demo only.\")\n",
    "                    os.makedirs(cdir, exist_ok=True)\n",
    "                    n = 8 if phase != 'train' else 64\n",
    "                    for i in range(n):\n",
    "                        img = Image.new('L', (Config.image_size, Config.image_size), color=random.randint(0, 255))\n",
    "                        img.convert('RGB').save(os.path.join(cdir, f'dummy_{i}.png'))\n",
    "                else:\n",
    "                    print(f\"[WARN] Missing class dir: {cdir}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "            files = [f for f in os.listdir(cdir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            if not files and Config.allow_make_dummy:\n",
    "                print(f\"[WARN] Empty {cdir}. Creating dummy images for demo only.\")\n",
    "                n = 8 if phase != 'train' else 64\n",
    "                for i in range(n):\n",
    "                    img = Image.new('L', (Config.image_size, Config.image_size), color=random.randint(0, 255))\n",
    "                    img.convert('RGB').save(os.path.join(cdir, f'dummy_{i}.png'))\n",
    "                files = [f for f in os.listdir(cdir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "            for name in files:\n",
    "                self.image_paths.append(os.path.join(cdir, name))\n",
    "                self.labels.append(idx)\n",
    "                # Extract patient ID from filename (assumes format: patientID_xxx.png)\n",
    "                # Modify this based on your actual naming convention\n",
    "                patient_id = name.split('_')[0] if '_' in name else name.split('.')[0]\n",
    "                self.patient_ids.append(f\"{cname}_{patient_id}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        p = self.image_paths[idx]\n",
    "        y = self.labels[idx]\n",
    "        pid = self.patient_ids[idx]\n",
    "        img = Image.open(p).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, y, idx, pid\n",
    "\n",
    "# Transforms (unchanged)\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize(int(Config.image_size*1.1)),\n",
    "    transforms.RandomResizedCrop(Config.image_size, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3), value='random')\n",
    "])\n",
    "\n",
    "val_test_tfms = transforms.Compose([\n",
    "    transforms.Resize((Config.image_size, Config.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = MRIDataset(Config.data_dir, 'train', train_tfms)\n",
    "val_dataset   = MRIDataset(Config.data_dir, 'validation', val_test_tfms)\n",
    "test_dataset  = MRIDataset(Config.data_dir, 'test', val_test_tfms)\n",
    "unseen_dataset= MRIDataset(Config.data_dir, 'unseen', val_test_tfms)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}, Unseen: {len(unseen_dataset)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# NEW: Patient-Level Split Verification\n",
    "# -------------------------------\n",
    "def verify_patient_level_split():\n",
    "    \"\"\"Verify no patient appears in multiple splits\"\"\"\n",
    "    train_patients = set(train_dataset.patient_ids)\n",
    "    val_patients = set(val_dataset.patient_ids)\n",
    "    test_patients = set(test_dataset.patient_ids)\n",
    "    \n",
    "    # Check for leakage\n",
    "    train_val_overlap = train_patients & val_patients\n",
    "    train_test_overlap = train_patients & test_patients\n",
    "    val_test_overlap = val_patients & test_patients\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PATIENT-LEVEL SPLIT VERIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Train set: {len(train_patients)} unique patients, {len(train_dataset)} images\")\n",
    "    print(f\"Validation set: {len(val_patients)} unique patients, {len(val_dataset)} images\")\n",
    "    print(f\"Test set: {len(test_patients)} unique patients, {len(test_dataset)} images\")\n",
    "    print(f\"Total unique patients: {len(train_patients | val_patients | test_patients)}\")\n",
    "    \n",
    "    if train_val_overlap:\n",
    "        print(f\"\\n⚠️  WARNING: {len(train_val_overlap)} patients appear in both TRAIN and VAL\")\n",
    "    if train_test_overlap:\n",
    "        print(f\"⚠️  WARNING: {len(train_test_overlap)} patients appear in both TRAIN and TEST\")\n",
    "    if val_test_overlap:\n",
    "        print(f\"⚠️  WARNING: {len(val_test_overlap)} patients appear in both VAL and TEST\")\n",
    "    \n",
    "    if not (train_val_overlap or train_test_overlap or val_test_overlap):\n",
    "        print(\"\\n✓ No data leakage detected: All splits are patient-level disjoint\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Save verification report\n",
    "    report = {\n",
    "        'Split': ['Train', 'Validation', 'Test', 'Total Unique'],\n",
    "        'Patients': [len(train_patients), len(val_patients), len(test_patients), \n",
    "                     len(train_patients | val_patients | test_patients)],\n",
    "        'Images': [len(train_dataset), len(val_dataset), len(test_dataset), \n",
    "                   len(train_dataset) + len(val_dataset) + len(test_dataset)]\n",
    "    }\n",
    "    df = pd.DataFrame(report)\n",
    "    df.to_csv(os.path.join(Config.output_dir, 'patient_split_verification.csv'), index=False)\n",
    "    print(f\"Saved split verification -> {os.path.join(Config.output_dir, 'patient_split_verification.csv')}\")\n",
    "\n",
    "if len(train_dataset) > 0:\n",
    "    verify_patient_level_split()\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"[ERROR] Train dataset is empty. Please prepare data under data/train/{AD,MCI,NC}.\")\n",
    "\n",
    "# Dataloaders\n",
    "def collate_fn(batch):\n",
    "    images, labels, indices, patient_ids = zip(*batch)\n",
    "    return torch.stack(images), torch.tensor(labels), torch.tensor(indices), list(patient_ids)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, \n",
    "                          num_workers=0, drop_last=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False, \n",
    "                          num_workers=0, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False, \n",
    "                          num_workers=0, collate_fn=collate_fn)\n",
    "unseen_loader= DataLoader(unseen_dataset, batch_size=Config.batch_size, shuffle=False, \n",
    "                          num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Model\n",
    "# -------------------------------\n",
    "class CustomInceptionV3(nn.Module):\n",
    "    def __init__(self, num_classes=3, dropout_rate=0.6, include_aux_logits=True, train_last_blocks_only=True):\n",
    "        super().__init__()\n",
    "        import torchvision\n",
    "        try:\n",
    "            self.inception_base = torchvision.models.inception_v3(\n",
    "                weights=torchvision.models.Inception_V3_Weights.IMAGENET1K_V1,\n",
    "                aux_logits=True\n",
    "            )\n",
    "        except Exception:\n",
    "            self.inception_base = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "        \n",
    "        self.inception_base.fc = nn.Identity()\n",
    "\n",
    "        if train_last_blocks_only:\n",
    "            for p in self.inception_base.parameters():\n",
    "                p.requires_grad = False\n",
    "            for name, m in self.inception_base.named_modules():\n",
    "                if name.startswith('Mixed_7'):\n",
    "                    for p in m.parameters():\n",
    "                        p.requires_grad = True\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(2048)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.hidden1 = nn.Linear(2048, 1024)\n",
    "        self.hidden2 = nn.Linear(1024, 512)\n",
    "        self.fc_head = nn.Linear(2048, num_classes)\n",
    "\n",
    "        self.mixed_7c_output = None\n",
    "        def hook_fn(_m, _in, out):\n",
    "            self.mixed_7c_output = out\n",
    "        self.inception_base.Mixed_7c.register_forward_hook(hook_fn)\n",
    "\n",
    "    def _extract_features(self, x):\n",
    "        out = self.inception_base(x)\n",
    "        if hasattr(out, 'logits'):\n",
    "            return out.logits\n",
    "        if isinstance(out, (tuple, list)) and len(out) > 0:\n",
    "            return out[0]\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        flatten = self._extract_features(x)\n",
    "        z = self.bn(flatten)\n",
    "        z = self.dropout(z)\n",
    "        logits = self.fc_head(z)\n",
    "        e = F.relu(self.hidden1(z))\n",
    "        e = F.relu(self.hidden2(e))\n",
    "        layer_dict = {\n",
    "            'base_model_output': self.mixed_7c_output,\n",
    "            'hidden_layer1': F.relu(self.hidden1(self.bn(flatten))),\n",
    "            'hidden_layer2': e,\n",
    "            'flatten': flatten,\n",
    "            'final_logits': logits\n",
    "        }\n",
    "        return logits, e, layer_dict\n",
    "\n",
    "# -------------------------------\n",
    "# 4. PPDRQ & Losses\n",
    "# -------------------------------\n",
    "\n",
    "def compute_ppdrq_from_logits(logits: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    if num_classes == 3:\n",
    "        p1, p2, p3 = probs[:, 0], probs[:, 1], probs[:, 2]\n",
    "        d12 = (p1 - p2).abs(); d13 = (p1 - p3).abs(); d23 = (p2 - p3).abs()\n",
    "        raw = (d12 + d13 + d23) / 3.0\n",
    "        pp = (3/2) * raw\n",
    "        return torch.clamp(pp, 0, 1)\n",
    "    diffs = []\n",
    "    for i in range(num_classes):\n",
    "        for j in range(i+1, num_classes):\n",
    "            diffs.append((probs[:, i] - probs[:, j]).abs())\n",
    "    sumdiff = torch.stack(diffs, dim=1).sum(dim=1)\n",
    "    pp = sumdiff / (num_classes - 1)\n",
    "    return torch.clamp(pp, 0, 1)\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, num_classes, lambda_triplet=Config.lambda_triplet, margin=Config.triplet_margin,\n",
    "                 epsilon_p=Config.epsilon_p, epsilon_n=Config.epsilon_n, smoothing=Config.label_smoothing):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_triplet = lambda_triplet\n",
    "        self.epsilon_p = epsilon_p\n",
    "        self.epsilon_n = epsilon_n\n",
    "        self.triplet = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, logits, embeddings, labels,\n",
    "                all_feats=None, all_labels=None, all_pp=None):\n",
    "        pp = compute_ppdrq_from_logits(logits, self.num_classes)\n",
    "        per_sample_ce = F.cross_entropy(logits, labels, reduction='none', label_smoothing=self.smoothing)\n",
    "        weights = 1 + (1 - pp)\n",
    "        ce_loss = torch.mean(per_sample_ce * weights)\n",
    "\n",
    "        triplet_loss = torch.tensor(0.0, device=logits.device)\n",
    "        if all_feats is not None and len(all_feats) > 0:\n",
    "            allF = torch.cat(all_feats, dim=0).to(embeddings.device)\n",
    "            allY = torch.cat(all_labels, dim=0).to(embeddings.device)\n",
    "            allP = torch.cat(all_pp, dim=0).to(embeddings.device)\n",
    "            count = 0\n",
    "            for i in range(labels.size(0)):\n",
    "                a = embeddings[i]\n",
    "                y = labels[i]\n",
    "                ppa = pp[i]\n",
    "                pos_idx = torch.where((allY == y) & ((allP - ppa).abs() < Config.epsilon_p))[0]\n",
    "                neg_idx = torch.where((allY != y) & ((allP - ppa).abs() >= Config.epsilon_n))[0]\n",
    "                if pos_idx.numel() > 1 and neg_idx.numel() > 0:\n",
    "                    p = allF[random.choice(pos_idx.tolist())]\n",
    "                    n = allF[random.choice(neg_idx.tolist())]\n",
    "                    triplet_loss = triplet_loss + self.triplet(a.unsqueeze(0), p.unsqueeze(0), n.unsqueeze(0))\n",
    "                    count += 1\n",
    "            if count > 0:\n",
    "                triplet_loss = triplet_loss / count\n",
    "        total = ce_loss + self.lambda_triplet * triplet_loss\n",
    "        return total, ce_loss, triplet_loss, pp\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Mixup/CutMix utils\n",
    "# -------------------------------\n",
    "\n",
    "def rand_bbox(W, H, lam):\n",
    "    cut_rat = math.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    x1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    y1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    x2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    y2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def apply_mixup_cutmix(x, y):\n",
    "    if not Config.use_mixup_cutmix:\n",
    "        return x, y, None\n",
    "    r = random.random()\n",
    "    if r < 0.5:\n",
    "        lam = np.random.beta(Config.mixup_alpha, Config.mixup_alpha)\n",
    "        idx = torch.randperm(x.size(0)).to(x.device)\n",
    "        mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "        y_a, y_b = y, y[idx]\n",
    "        return mixed_x, (y_a, y_b, lam), 'mixup'\n",
    "    else:\n",
    "        lam = np.random.beta(Config.cutmix_alpha, Config.cutmix_alpha)\n",
    "        idx = torch.randperm(x.size(0)).to(x.device)\n",
    "        x1, y1, x2, y2 = rand_bbox(x.size(3), x.size(2), lam)\n",
    "        x_mix = x.clone()\n",
    "        x_mix[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]\n",
    "        lam = 1 - ((x2 - x1) * (y2 - y1) / (x.size(-1) * x.size(-2)))\n",
    "        y_a, y_b = y, y[idx]\n",
    "        return x_mix, (y_a, y_b, lam), 'cutmix'\n",
    "\n",
    "\n",
    "def mix_criterion(logits, target_tuple):\n",
    "    y_a, y_b, lam = target_tuple\n",
    "    loss_a = F.cross_entropy(logits, y_a, label_smoothing=Config.label_smoothing)\n",
    "    loss_b = F.cross_entropy(logits, y_b, label_smoothing=Config.label_smoothing)\n",
    "    return lam * loss_a + (1 - lam) * loss_b\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Train/Eval\n",
    "# -------------------------------\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, model_name, num_epochs=Config.epochs):\n",
    "    best_val = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'train_ppdrq': [], 'val_ppdrq': [], 'epochs_trained': 0}\n",
    "    start = time.time()\n",
    "\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        t_loss = 0.0; t_correct = 0; t_total = 0; t_pp = 0.0\n",
    "\n",
    "        # Pre-collect for triplet\n",
    "        allF, allY, allP = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _, _ in train_loader:\n",
    "                xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device)\n",
    "                logits, emb, _ = model(xb)\n",
    "                pp = compute_ppdrq_from_logits(logits, Config.num_classes)\n",
    "                allF.append(emb.detach().cpu()); allY.append(yb.detach().cpu()); allP.append(pp.detach().cpu())\n",
    "        allF = torch.cat(allF, 0) if allF else torch.tensor([])\n",
    "        allY = torch.cat(allY, 0) if allY else torch.tensor([])\n",
    "        allP = torch.cat(allP, 0) if allP else torch.tensor([])\n",
    "\n",
    "        for xb, yb, _, _ in train_loader:\n",
    "            xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device)\n",
    "            xb_m, yb_m, aug = apply_mixup_cutmix(xb, yb)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits, emb, _ = model(xb_m)\n",
    "\n",
    "            if aug is None:\n",
    "                total, ce, trip, pp = criterion(logits, emb, yb, [allF], [allY], [allP])\n",
    "            else:\n",
    "                ce_mix = mix_criterion(logits, yb_m)\n",
    "                pp = compute_ppdrq_from_logits(logits, Config.num_classes)\n",
    "                total = ce_mix\n",
    "                ce, trip = ce_mix, torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "            total.backward()\n",
    "            if Config.grad_clip is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), Config.grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                pred = probs.argmax(1)\n",
    "                if aug is None:\n",
    "                    t_correct += (pred == yb).sum().item()\n",
    "                    t_total += yb.size(0)\n",
    "                else:\n",
    "                    y_a, _, lam = yb_m\n",
    "                    t_correct += (pred == y_a).sum().item() * lam\n",
    "                    t_total += y_a.size(0)\n",
    "                t_loss += total.item() * xb.size(0)\n",
    "                t_pp += pp.sum().item()\n",
    "\n",
    "        tr_loss = t_loss / max(t_total, 1)\n",
    "        tr_acc = t_correct / max(t_total, 1)\n",
    "        tr_pp = t_pp / max(t_total, 1)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        v_loss=0.0; v_cor=0; v_tot=0; v_pp=0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _, _ in val_loader:\n",
    "                xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device)\n",
    "                logits, emb, _ = model(xb)\n",
    "                total, ce, trip, pp = criterion(logits, emb, yb)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                pred = probs.argmax(1)\n",
    "                v_cor += (pred == yb).sum().item()\n",
    "                v_tot += yb.size(0)\n",
    "                v_loss += total.item() * xb.size(0)\n",
    "                v_pp += pp.sum().item()\n",
    "        va_loss = v_loss / max(v_tot, 1)\n",
    "        va_acc  = v_cor / max(v_tot, 1)\n",
    "        va_pp   = v_pp / max(v_tot, 1)\n",
    "\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['val_loss'].append(va_loss)\n",
    "        history['train_acc'].append(tr_acc)\n",
    "        history['val_acc'].append(va_acc)\n",
    "        history['train_ppdrq'].append(tr_pp)\n",
    "        history['val_ppdrq'].append(va_pp)\n",
    "        history['epochs_trained'] = epoch + 1\n",
    "\n",
    "        if (epoch+1) == 1 or (epoch+1) % 5 == 0 or (epoch+1) == num_epochs:\n",
    "            print(f\"Epoch {epoch+1:03d}/{num_epochs} | Train: loss {tr_loss:.4f} acc {tr_acc:.4f} pp {tr_pp:.3f} | Val: loss {va_loss:.4f} acc {va_acc:.4f} pp {va_pp:.3f}\")\n",
    "\n",
    "        scheduler.step(va_loss)\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), os.path.join(Config.output_dir, f\"{model_name}_best.pth\"))\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= Config.patience_early_stopping:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    dur = time.time() - start\n",
    "    print(f\"Training time for {model_name}: {dur:.1f}s\")\n",
    "    return history, dur\n",
    "\n",
    "\n",
    "def calculate_calibration_metrics(probabilities: torch.Tensor, labels: torch.Tensor, num_bins=10):\n",
    "    bins = torch.linspace(0, 1, num_bins + 1)\n",
    "    conf, pred = probabilities.max(1)\n",
    "    acc = (pred == labels).float()\n",
    "    ece = 0.0\n",
    "    for i in range(num_bins):\n",
    "        in_bin = (conf > bins[i]) & (conf <= bins[i+1])\n",
    "        if in_bin.any():\n",
    "            ece += torch.abs(acc[in_bin].mean() - conf[in_bin].mean()) * in_bin.float().mean()\n",
    "    one_hot = F.one_hot(labels, num_classes=Config.num_classes).float()\n",
    "    brier = torch.mean(torch.sum((probabilities - one_hot) ** 2, dim=1))\n",
    "    return ece.item(), brier.item()\n",
    "\n",
    "\n",
    "def plot_reliability_diagram(conf, correct, ece, model_name, num_bins=10):\n",
    "    bins = np.linspace(0, 1, num_bins + 1)\n",
    "    mids = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_acc = []\n",
    "    counts = []\n",
    "    for i in range(num_bins):\n",
    "        m = (conf >= bins[i]) & (conf <= bins[i+1])\n",
    "        if m.sum() > 0:\n",
    "            bin_acc.append(correct[m].float().mean().item())\n",
    "            counts.append(int(m.sum()))\n",
    "        else:\n",
    "            bin_acc.append(0.0); counts.append(0)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot([0,1],[0,1],'k:')\n",
    "    plt.bar(mids, bin_acc, width=1/num_bins*0.9, alpha=0.7, edgecolor='black')\n",
    "    for i, c in enumerate(counts):\n",
    "        if c>0:\n",
    "            plt.text(mids[i], bin_acc[i]+0.02, str(c), ha='center', fontsize=8)\n",
    "    plt.title(f\"Reliability: {model_name} (ECE={ece:.3f})\")\n",
    "    plt.xlabel('Confidence'); plt.ylabel('Accuracy'); plt.ylim(0,1)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(Config.output_dir, f\"{model_name}_reliability.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, model_name):\n",
    "    model.eval()\n",
    "    all_prob=[]; all_y=[]\n",
    "    t0=time.time()\n",
    "    if len(loader.dataset)==0:\n",
    "        print(f\"[WARN] {model_name} loader empty.\")\n",
    "        return {k:0.0 for k in ['accuracy','precision','recall','f1_score','mean_ppdrq','ece','brier_score','inference_time']}\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _, _ in tqdm(loader, desc=f\"Evaluating {model_name}\"):\n",
    "            xb=xb.to(Config.device); yb=torch.as_tensor(yb, device=Config.device)\n",
    "            logits, _, _ = model(xb)\n",
    "            prob = torch.softmax(logits, dim=1)\n",
    "            all_prob.append(prob.cpu()); all_y.append(yb.cpu())\n",
    "    t1=time.time()\n",
    "    prob=torch.cat(all_prob,0)\n",
    "    y=torch.cat(all_y,0)\n",
    "    pred=prob.argmax(1)\n",
    "    acc=accuracy_score(y.numpy(), pred.numpy())\n",
    "    prec=precision_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    rec=recall_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    f1=f1_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    pp = compute_ppdrq_from_logits(torch.log(prob+1e-8), Config.num_classes)\n",
    "    ece,brier=calculate_calibration_metrics(prob, y)\n",
    "    plot_reliability_diagram(prob.max(1)[0], (pred==y), ece, model_name)\n",
    "    print(f\"\\n[{model_name}] Acc {acc:.4f} Prec {prec:.4f} Rec {rec:.4f} F1 {f1:.4f} PPDRQ {pp.mean().item():.4f} ECE {ece:.4f} Brier {brier:.4f}\")\n",
    "    return {\n",
    "        'accuracy':acc,'precision':prec,'recall':rec,'f1_score':f1,\n",
    "        'mean_ppdrq':pp.mean().item(),'ece':ece,'brier_score':brier,\n",
    "        'inference_time':t1-t0\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# NEW: Statistical Significance Testing\n",
    "# -------------------------------\n",
    "\n",
    "def bootstrap_confidence_interval(y_true, y_pred, metric_fn, n_bootstrap=Config.bootstrap_iterations):\n",
    "    \"\"\"Calculate bootstrap confidence intervals for a metric\"\"\"\n",
    "    scores = []\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        score = metric_fn(y_true[indices], y_pred[indices])\n",
    "        scores.append(score)\n",
    "    \n",
    "    ci_lower = np.percentile(scores, (1 - Config.confidence_level) / 2 * 100)\n",
    "    ci_upper = np.percentile(scores, (1 + Config.confidence_level) / 2 * 100)\n",
    "    mean_score = np.mean(scores)\n",
    "    \n",
    "    return mean_score, ci_lower, ci_upper\n",
    "\n",
    "\n",
    "def mcnemar_test(y_true, pred_model1, pred_model2):\n",
    "    \"\"\"Perform McNemar's test for paired predictions\"\"\"\n",
    "    correct1 = (pred_model1 == y_true)\n",
    "    correct2 = (pred_model2 == y_true)\n",
    "    \n",
    "    # Contingency table\n",
    "    n01 = np.sum(correct1 & ~correct2)  # Model1 correct, Model2 wrong\n",
    "    n10 = np.sum(~correct1 & correct2)  # Model1 wrong, Model2 correct\n",
    "    \n",
    "    # McNemar's test statistic with continuity correction\n",
    "    if (n01 + n10) == 0:\n",
    "        return 1.0  # No difference\n",
    "    \n",
    "    statistic = (abs(n01 - n10) - 1) ** 2 / (n01 + n10)\n",
    "    p_value = 1 - stats.chi2.cdf(statistic, df=1)\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "\n",
    "def compute_statistical_comparisons(models_predictions: Dict[str, Tuple], baseline_name='Baseline'):\n",
    "    \"\"\"\n",
    "    Compare all models against baseline with statistical tests\n",
    "    \n",
    "    Args:\n",
    "        models_predictions: Dict[model_name] = (y_true, y_pred, probabilities)\n",
    "        baseline_name: Name of baseline model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = []\n",
    "    baseline_y_true, baseline_y_pred, _ = models_predictions[baseline_name]\n",
    "    \n",
    "    # Metrics to test\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score,\n",
    "        'F1-Score': lambda yt, yp: f1_score(yt, yp, average='macro', zero_division=0),\n",
    "        'Precision': lambda yt, yp: precision_score(yt, yp, average='macro', zero_division=0),\n",
    "        'Recall': lambda yt, yp: recall_score(yt, yp, average='macro', zero_division=0)\n",
    "    }\n",
    "    \n",
    "    for model_name, (y_true, y_pred, probs) in models_predictions.items():\n",
    "        print(f\"\\n--- {model_name} vs {baseline_name} ---\")\n",
    "        \n",
    "        model_results = {'Model': model_name}\n",
    "        \n",
    "        # Bootstrap CI for each metric\n",
    "        for metric_name, metric_fn in metrics.items():\n",
    "            mean_score, ci_lower, ci_upper = bootstrap_confidence_interval(\n",
    "                y_true, y_pred, metric_fn, n_bootstrap=Config.bootstrap_iterations\n",
    "            )\n",
    "            print(f\"{metric_name}: {mean_score:.4f} (95% CI: [{ci_lower:.4f}, {ci_upper:.4f}])\")\n",
    "            model_results[f'{metric_name}_Mean'] = mean_score\n",
    "            model_results[f'{metric_name}_CI_Lower'] = ci_lower\n",
    "            model_results[f'{metric_name}_CI_Upper'] = ci_upper\n",
    "        \n",
    "        # McNemar's test against baseline\n",
    "        if model_name != baseline_name:\n",
    "            p_value = mcnemar_test(y_true, y_pred, baseline_y_pred)\n",
    "            is_significant = p_value < 0.05\n",
    "            print(f\"\\nMcNemar's Test p-value: {p_value:.4f} {'(Significant)' if is_significant else '(Not Significant)'}\")\n",
    "            model_results['McNemar_p_value'] = p_value\n",
    "            model_results['Significant_vs_Baseline'] = is_significant\n",
    "        else:\n",
    "            model_results['McNemar_p_value'] = np.nan\n",
    "            model_results['Significant_vs_Baseline'] = np.nan\n",
    "        \n",
    "        results.append(model_results)\n",
    "    \n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    csv_path = os.path.join(Config.output_dir, 'statistical_significance_results.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n✓ Saved statistical results -> {csv_path}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_confidence_intervals(stat_df: pd.DataFrame, metric='Accuracy'):\n",
    "    \"\"\"Plot confidence intervals for comparison\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    models = stat_df['Model'].values\n",
    "    means = stat_df[f'{metric}_Mean'].values\n",
    "    ci_lowers = stat_df[f'{metric}_CI_Lower'].values\n",
    "    ci_uppers = stat_df[f'{metric}_CI_Upper'].values\n",
    "    \n",
    "    y_pos = np.arange(len(models))\n",
    "    errors = np.array([means - ci_lowers, ci_uppers - means])\n",
    "    \n",
    "    ax.errorbar(means, y_pos, xerr=errors, fmt='o', markersize=8, capsize=5, capthick=2)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(models)\n",
    "    ax.set_xlabel(f'{metric} Score')\n",
    "    ax.set_title(f'{metric} with 95% Confidence Intervals')\n",
    "    ax.grid(True, axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(Config.output_dir, f'{metric.lower()}_confidence_intervals.png')\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved confidence interval plot -> {out_path}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Layer-wise PPDRQ\n",
    "# -------------------------------\n",
    "\n",
    "def get_and_plot_layerwise_ppdrq(model, loader, model_name, title_suffix=\"\"):\n",
    "    model.eval()\n",
    "    collect = {k:[] for k in ['base_model_output','hidden_layer1','hidden_layer2','flatten','final_logits']}\n",
    "    if len(loader.dataset)==0:\n",
    "        print(f\"[WARN] No data for layer-wise PPDRQ.\")\n",
    "        return\n",
    "    with torch.no_grad():\n",
    "        for xb, _, _, _ in loader:\n",
    "            xb=xb.to(Config.device)\n",
    "            logits, _, d = model(xb)\n",
    "            for k,v in d.items():\n",
    "                if k=='base_model_output':\n",
    "                    collect[k].append(v.mean(dim=(2,3)).cpu())\n",
    "                else:\n",
    "                    collect[k].append(v.cpu())\n",
    "    temp_base = nn.Linear(2048, Config.num_classes)\n",
    "    temp_h1   = nn.Linear(1024, Config.num_classes)\n",
    "    temp_h2   = nn.Linear(512, Config.num_classes)\n",
    "    temp_flat = nn.Linear(2048, Config.num_classes)\n",
    "    mean_vals={}\n",
    "    for k, arr in collect.items():\n",
    "        if not arr: mean_vals[k]=0.0; continue\n",
    "        X = torch.cat(arr,0)\n",
    "        if k=='base_model_output':\n",
    "            l = temp_base(X)\n",
    "        elif k=='hidden_layer1':\n",
    "            l = temp_h1(X)\n",
    "        elif k=='hidden_layer2':\n",
    "            l = temp_h2(X)\n",
    "        elif k=='flatten':\n",
    "            l = temp_flat(X)\n",
    "        else:\n",
    "            l = X\n",
    "        pp = compute_ppdrq_from_logits(l, Config.num_classes)\n",
    "        mean_vals[k]=pp.mean().item()\n",
    "    mmax=max(mean_vals.values()) if mean_vals else 1.0\n",
    "    norm={k:(v/mmax if mmax>0 else 0.0) for k,v in mean_vals.items()}\n",
    "    layers=list(norm.keys()); vals=[norm[k] for k in layers]\n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.bar(layers, vals)\n",
    "    for i,v in enumerate(vals):\n",
    "        plt.text(i, v+0.02, f\"{v:.2f}\", ha='center')\n",
    "    plt.ylim(0,1); plt.ylabel('Normalized Mean PPDRQ'); plt.title(f\"Layer PPDRQ {title_suffix}\")\n",
    "    plt.savefig(os.path.join(Config.output_dir, f\"{model_name}_layer_ppdrq.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------------\n",
    "# 8. FIXED MC-Dropout Implementation\n",
    "# -------------------------------\n",
    "\n",
    "def enable_dropout(model):\n",
    "    \"\"\"Enable dropout layers during inference\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.train()\n",
    "\n",
    "\n",
    "def mc_dropout_predict(model, loader, num_passes=Config.mc_dropout_passes):\n",
    "    \"\"\"\n",
    "    FIXED MC-Dropout implementation:\n",
    "    - Ensures dropout stays active during inference\n",
    "    - Uses model.train() mode for dropout layers only\n",
    "    \"\"\"\n",
    "    print(f\"\\n[MC-Dropout] Running {num_passes} stochastic forward passes...\")\n",
    "    \n",
    "    # Put model in eval mode first\n",
    "    model.eval()\n",
    "    # Then explicitly enable dropout\n",
    "    enable_dropout(model)\n",
    "    \n",
    "    all_prob=[]; all_y=[]; t0=time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _, _ in tqdm(loader, desc=f\"MC-Dropout {num_passes} passes\"):\n",
    "            xb=xb.to(Config.device); yb=torch.as_tensor(yb, device=Config.device)\n",
    "            batch_predictions = []\n",
    "            \n",
    "            # Multiple stochastic forward passes\n",
    "            for pass_idx in range(num_passes):\n",
    "                logits, _, _ = model(xb)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                batch_predictions.append(probs.unsqueeze(0))\n",
    "            \n",
    "            # Average predictions across passes\n",
    "            mean_prob = torch.cat(batch_predictions, 0).mean(0)\n",
    "            all_prob.append(mean_prob.cpu())\n",
    "            all_y.append(yb.cpu())\n",
    "    \n",
    "    t1=time.time()\n",
    "    \n",
    "    # Back to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    prob=torch.cat(all_prob,0); y=torch.cat(all_y,0)\n",
    "    pred=prob.argmax(1)\n",
    "    \n",
    "    acc=accuracy_score(y.numpy(), pred.numpy())\n",
    "    prec=precision_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    rec=recall_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    f1=f1_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    ece,brier=calculate_calibration_metrics(prob, y)\n",
    "    plot_reliability_diagram(prob.max(1)[0], (pred==y), ece, 'MC-Dropout')\n",
    "    pp = compute_ppdrq_from_logits(torch.log(prob+1e-8), Config.num_classes)\n",
    "    \n",
    "    print(f\"\\n[MC-Dropout] Acc {acc:.4f} Prec {prec:.4f} Rec {rec:.4f} F1 {f1:.4f} PPDRQ {pp.mean().item():.4f} ECE {ece:.4f} Brier {brier:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy':acc,'precision':prec,'recall':rec,'f1_score':f1,\n",
    "        'mean_ppdrq':pp.mean().item(),'ece':ece,'brier_score':brier,\n",
    "        'inference_time':t1-t0\n",
    "    }\n",
    "\n",
    "\n",
    "def train_ensemble_member(i, train_loader, val_loader):\n",
    "    print(f\"\\n--- Training Ensemble Member {i+1} ---\")\n",
    "    m = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device)\n",
    "    crit = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=0.0)\n",
    "    opt = optim.AdamW(filter(lambda p: p.requires_grad, m.parameters()), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    sch = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.2, patience=Config.patience_lr_scheduler)\n",
    "    hist, t = train_model(m, train_loader, val_loader, crit, opt, sch, f\"Ensemble_{i+1}\", num_epochs=max(10, Config.epochs//Config.num_ensemble_models))\n",
    "    torch.save(m.state_dict(), os.path.join(Config.output_dir, f\"ensemble_member_{i+1}.pth\"))\n",
    "    return m, t, hist\n",
    "\n",
    "\n",
    "def evaluate_deep_ensemble(models_list: List[nn.Module], loader):\n",
    "    all_prob=[]; all_y=[]; t0=time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _, _ in tqdm(loader, desc=\"Evaluating Ensemble\"):\n",
    "            xb=xb.to(Config.device)\n",
    "            mem_probs=[]\n",
    "            for m in models_list:\n",
    "                m.eval()\n",
    "                logits, _, _ = m(xb)\n",
    "                mem_probs.append(torch.softmax(logits, dim=1).unsqueeze(0))\n",
    "            mean_prob=torch.cat(mem_probs,0).mean(0)\n",
    "            all_prob.append(mean_prob.cpu()); all_y.append(torch.as_tensor(yb).cpu())\n",
    "    t1=time.time()\n",
    "    prob=torch.cat(all_prob,0); y=torch.cat(all_y,0)\n",
    "    pred=prob.argmax(1)\n",
    "    acc=accuracy_score(y.numpy(), pred.numpy())\n",
    "    prec=precision_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    rec=recall_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    f1=f1_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    ece,brier=calculate_calibration_metrics(prob, y)\n",
    "    plot_reliability_diagram(prob.max(1)[0], (pred==y), ece, 'Deep-Ensemble')\n",
    "    pp = compute_ppdrq_from_logits(torch.log(prob+1e-8), Config.num_classes)\n",
    "    print(f\"\\n[Ensemble] Acc {acc:.4f} F1 {f1:.4f} PPDRQ {pp.mean().item():.4f} ECE {ece:.4f}\")\n",
    "    return {'accuracy':acc,'precision':prec,'recall':rec,'f1_score':f1,\n",
    "            'mean_ppdrq':pp.mean().item(),'ece':ece,'brier_score':brier,\n",
    "            'inference_time':t1-t0}\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Paper-Ready Reporting Utilities\n",
    "# -------------------------------\n",
    "\n",
    "def save_results_table(results: Dict[str, Dict], times: Dict[str, float], path_csv: str):\n",
    "    rows = []\n",
    "    for name, m in results.items():\n",
    "        train_t = times.get(name, times.get(f\"{name}_Train\", np.nan))\n",
    "        infer_t = m.get('inference_time', times.get(f\"{name}_Infer\", np.nan))\n",
    "        rows.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': m['accuracy'], 'Precision': m['precision'], 'Recall': m['recall'], 'F1': m['f1_score'],\n",
    "            'Mean_PPDRQ': m['mean_ppdrq'], 'ECE': m['ece'], 'Brier': m['brier_score'],\n",
    "            'Train_Time_s': train_t, 'Infer_Time_s': infer_t\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    csv_path = os.path.join(Config.output_dir, path_csv)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved results table -> {csv_path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_metric_bars(df: pd.DataFrame, metric: str, fname: str):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    order = df.sort_values(metric, ascending=False)['Model']\n",
    "    plt.bar(order, df.set_index('Model').loc[order, metric])\n",
    "    plt.xticks(rotation=25, ha='right')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f\"Model comparison: {metric}\")\n",
    "    out = os.path.join(Config.output_dir, fname)\n",
    "    plt.tight_layout(); plt.savefig(out); plt.close()\n",
    "    print(f\"Saved {metric} bar plot -> {out}\")\n",
    "\n",
    "\n",
    "def plot_cost_performance(df: pd.DataFrame, perf_metric: str='Accuracy', cost_metric: str='Infer_Time_s', fname: str='cost_perf.png'):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    x = df[cost_metric].values; y = df[perf_metric].values\n",
    "    for i, row in df.iterrows():\n",
    "        plt.scatter(row[cost_metric], row[perf_metric])\n",
    "        plt.text(row[cost_metric], row[perf_metric]+0.005, row['Model'], fontsize=8)\n",
    "    plt.xlabel(cost_metric); plt.ylabel(perf_metric)\n",
    "    plt.title(f\"Performance-Cost Trade-off ({perf_metric} vs {cost_metric})\")\n",
    "    out = os.path.join(Config.output_dir, fname)\n",
    "    plt.grid(True); plt.tight_layout(); plt.savefig(out); plt.close()\n",
    "    print(f\"Saved cost-performance plot -> {out}\")\n",
    "\n",
    "\n",
    "def lambda_sensitivity_sweep(lambdas=(0.0, 0.01, 0.1, 0.3, 1.0)):\n",
    "    \"\"\"Re-train with different λ (triplet weight) and plot Accuracy/ECE vs λ\"\"\"\n",
    "    acc_list=[]; ece_list=[]\n",
    "    for lam in lambdas:\n",
    "        print(f\"\\n[λ-sweep] Training with λ={lam}\")\n",
    "        model = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device)\n",
    "        crit = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=lam)\n",
    "        opt = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "        sch = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.2, patience=Config.patience_lr_scheduler)\n",
    "        hist, _ = train_model(model, train_loader, val_loader, crit, opt, sch, f'Lambda_{lam}', num_epochs=max(5, Config.epochs//6))\n",
    "        res = evaluate_model(model, test_loader, f'Lambda_{lam}')\n",
    "        acc_list.append(res['accuracy']); ece_list.append(res['ece'])\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(list(lambdas), acc_list, marker='o', label='Accuracy')\n",
    "    plt.plot(list(lambdas), ece_list, marker='o', label='ECE')\n",
    "    plt.xlabel('λ (triplet weight)'); plt.legend(); plt.grid(True)\n",
    "    out = os.path.join(Config.output_dir, 'lambda_sensitivity.png')\n",
    "    plt.tight_layout(); plt.savefig(out); plt.close()\n",
    "    print(f\"Saved λ sensitivity plot -> {out}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9b. Visualization Functions\n",
    "# -------------------------------\n",
    "CLASS_NAMES = ['AD','MCI','NC']\n",
    "\n",
    "def collect_predictions(model, loader):\n",
    "    y_true=[]; y_pred=[]; conf=[]; probs=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _, _ in loader:\n",
    "            xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device)\n",
    "            logits, _, _ = model(xb)\n",
    "            p = torch.softmax(logits, dim=1)\n",
    "            pr = p.argmax(1)\n",
    "            y_true.append(yb.cpu()); y_pred.append(pr.cpu()); conf.append(p.max(1)[0].cpu()); probs.append(p.cpu())\n",
    "    return torch.cat(y_true).numpy(), torch.cat(y_pred).numpy(), torch.cat(conf).numpy(), torch.cat(probs,0).numpy()\n",
    "\n",
    "\n",
    "def collect_predictions_mc_dropout(model, loader, num_passes=Config.mc_dropout_passes):\n",
    "    \"\"\"Collect predictions using MC-Dropout\"\"\"\n",
    "    y_true=[]; y_pred=[]; conf=[]; probs=[]\n",
    "    \n",
    "    model.eval()\n",
    "    enable_dropout(model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _, _ in loader:\n",
    "            xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device)\n",
    "            batch_preds = []\n",
    "            for _ in range(num_passes):\n",
    "                logits, _, _ = model(xb)\n",
    "                batch_preds.append(torch.softmax(logits, dim=1).unsqueeze(0))\n",
    "            p = torch.cat(batch_preds, 0).mean(0)\n",
    "            pr = p.argmax(1)\n",
    "            y_true.append(yb.cpu()); y_pred.append(pr.cpu()); conf.append(p.max(1)[0].cpu()); probs.append(p.cpu())\n",
    "    \n",
    "    model.eval()\n",
    "    return torch.cat(y_true).numpy(), torch.cat(y_pred).numpy(), torch.cat(conf).numpy(), torch.cat(probs,0).numpy()\n",
    "\n",
    "\n",
    "def plot_confusion(y_true, y_pred, classes, title, fname, normalize=True):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True).clip(min=1)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title(title); plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45); plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, f\"{cm[i, j]:.2f}\" if normalize else int(cm[i, j]),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True'); plt.xlabel('Predicted'); plt.tight_layout()\n",
    "    out = os.path.join(Config.output_dir, fname)\n",
    "    plt.savefig(out); plt.close(); print(f\"Saved confusion matrix -> {out}\")\n",
    "\n",
    "\n",
    "def export_per_class_report(y_true, y_pred, classes, fname_csv):\n",
    "    rep = classification_report(y_true, y_pred, target_names=classes, output_dict=True, zero_division=0)\n",
    "    df = pd.DataFrame(rep).transpose()\n",
    "    out = os.path.join(Config.output_dir, fname_csv)\n",
    "    df.to_csv(out)\n",
    "    print(f\"Saved per-class report -> {out}\")\n",
    "\n",
    "\n",
    "def reliability_grid(models_dict, loader, fname='reliability_grid.png', num_bins=10):\n",
    "    cols = len(models_dict); plt.figure(figsize=(5*cols,5))\n",
    "    for idx,(name, model) in enumerate(models_dict.items(), start=1):\n",
    "        model.eval(); all_prob=[]; all_y=[]\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _, _ in loader:\n",
    "                xb=xb.to(Config.device); yb=torch.as_tensor(yb, device=Config.device)\n",
    "                logits, _, _ = model(xb); pr = torch.softmax(logits, dim=1)\n",
    "                all_prob.append(pr.cpu()); all_y.append(yb.cpu())\n",
    "        prob=torch.cat(all_prob,0); y=torch.cat(all_y,0); pred=prob.argmax(1)\n",
    "        conf=prob.max(1)[0]; correct=(pred==y)\n",
    "        ece,_ = calculate_calibration_metrics(prob, y)\n",
    "        bins = np.linspace(0,1,num_bins+1); mids=(bins[:-1]+bins[1:])/2\n",
    "        bin_acc=[]\n",
    "        for i in range(num_bins):\n",
    "            m = (conf>=bins[i]) & (conf<=bins[i+1])\n",
    "            bin_acc.append(correct[m].float().mean().item() if m.sum()>0 else 0.0)\n",
    "        ax = plt.subplot(1, cols, idx)\n",
    "        ax.plot([0,1],[0,1],'k:'); ax.bar(mids, bin_acc, width=1/num_bins*0.9, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title(f\"{name} (ECE={ece:.3f})\"); ax.set_xlabel('Confidence'); ax.set_ylabel('Accuracy'); ax.set_ylim(0,1); ax.grid(True)\n",
    "    out = os.path.join(Config.output_dir, fname)\n",
    "    plt.tight_layout(); plt.savefig(out); plt.close(); print(f\"Saved reliability grid -> {out}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# NEW: Clinical Validation Framework\n",
    "# -------------------------------\n",
    "\n",
    "class ClinicalValidationFramework:\n",
    "    \"\"\"Framework for systematic clinical validation\"\"\"\n",
    "    \n",
    "    def __init__(self, model, test_loader, model_name):\n",
    "        self.model = model\n",
    "        self.test_loader = test_loader\n",
    "        self.model_name = model_name\n",
    "        self.selected_cases = []\n",
    "        \n",
    "    def select_cases_for_review(self, n_high_conf=10, n_low_conf=10, n_errors=10):\n",
    "        \"\"\"Select representative cases for neurologist review\"\"\"\n",
    "        print(f\"\\n[Clinical Validation] Selecting cases for {self.model_name}...\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        all_cases = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xb, yb, indices, patient_ids in self.test_loader:\n",
    "                xb = xb.to(Config.device)\n",
    "                yb = torch.as_tensor(yb, device=Config.device)\n",
    "                logits, _, _ = self.model(xb)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                preds = probs.argmax(1)\n",
    "                confs = probs.max(1)[0]\n",
    "                \n",
    "                # Compute uncertainty (PPDRQ)\n",
    "                pp = compute_ppdrq_from_logits(logits, Config.num_classes)\n",
    "                \n",
    "                for i in range(len(yb)):\n",
    "                    all_cases.append({\n",
    "                        'patient_id': patient_ids[i],\n",
    "                        'true_label': CLASS_NAMES[yb[i].item()],\n",
    "                        'pred_label': CLASS_NAMES[preds[i].item()],\n",
    "                        'confidence': confs[i].item(),\n",
    "                        'uncertainty_ppdrq': pp[i].item(),\n",
    "                        'correct': (preds[i] == yb[i]).item(),\n",
    "                        'index': indices[i].item()\n",
    "                    })\n",
    "        \n",
    "        # Categorize cases\n",
    "        correct_high_conf = sorted([c for c in all_cases if c['correct'] and c['confidence'] > 0.8],\n",
    "                                   key=lambda x: -x['confidence'])[:n_high_conf]\n",
    "        \n",
    "        correct_low_conf = sorted([c for c in all_cases if c['correct'] and c['uncertainty_ppdrq'] > 0.3],\n",
    "                                 key=lambda x: -x['uncertainty_ppdrq'])[:n_low_conf]\n",
    "        \n",
    "        errors = sorted([c for c in all_cases if not c['correct']],\n",
    "                       key=lambda x: -x['confidence'])[:n_errors]\n",
    "        \n",
    "        self.selected_cases = {\n",
    "            'high_confidence_correct': correct_high_conf,\n",
    "            'low_confidence_correct': correct_low_conf,\n",
    "            'errors': errors\n",
    "        }\n",
    "        \n",
    "        total = len(correct_high_conf) + len(correct_low_conf) + len(errors)\n",
    "        print(f\"Selected {total} cases for review:\")\n",
    "        print(f\"  - {len(correct_high_conf)} high-confidence correct predictions\")\n",
    "        print(f\"  - {len(correct_low_conf)} low-confidence correct predictions (high uncertainty)\")\n",
    "        print(f\"  - {len(errors)} misclassifications\")\n",
    "        \n",
    "        return self.selected_cases\n",
    "    \n",
    "    def generate_validation_report(self, output_file='clinical_validation_cases.csv'):\n",
    "        \"\"\"Generate report for neurologist review\"\"\"\n",
    "        all_cases_flat = []\n",
    "        \n",
    "        for category, cases in self.selected_cases.items():\n",
    "            for case in cases:\n",
    "                case_copy = case.copy()\n",
    "                case_copy['category'] = category\n",
    "                all_cases_flat.append(case_copy)\n",
    "        \n",
    "        df = pd.DataFrame(all_cases_flat)\n",
    "        csv_path = os.path.join(Config.output_dir, output_file)\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\n✓ Saved clinical validation cases -> {csv_path}\")\n",
    "        print(f\"Total cases for neurologist review: {len(all_cases_flat)}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def print_example_observations(self):\n",
    "        \"\"\"Print template for clinical observations\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CLINICAL VALIDATION TEMPLATE\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\"\"\n",
    "Number of cases inspected by neurologist: [TO BE FILLED]\n",
    "\n",
    "Clinical Observations:\n",
    "\n",
    "1. High Uncertainty Concordance:\n",
    "   In X out of Y cases with high model uncertainty (PPDRQ > 0.3), these corresponded\n",
    "   to clinically ambiguous features such as:\n",
    "   - Boundary ambiguity in hippocampal atrophy assessment\n",
    "   - Subtle white matter hyperintensities difficult to classify\n",
    "   - [Add specific observations]\n",
    "\n",
    "2. Error Analysis:\n",
    "   Model misclassifications in Z cases showed patterns of:\n",
    "   - Confusion between MCI and early AD in cases with moderate- Confusion between MCI and early AD in cases with moderate atrophy\n",
    "   - Motion artifacts affecting model confidence appropriately\n",
    "   - [Add specific observations]\n",
    "\n",
    "3. Clinical Concordance:\n",
    "   Model showed appropriate high uncertainty in cases that were also\n",
    "   challenging for clinical assessment, particularly in:\n",
    "   - [Region/pathology 1]\n",
    "   - [Region/pathology 2]\n",
    "   \n",
    "Note: This template should be filled by reviewing neurologist after examining\n",
    "the selected cases listed in clinical_validation_cases.csv\n",
    "        \"\"\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Main Execution\n",
    "# -------------------------------\n",
    "if __name__ == '__main__':\n",
    "    results: Dict[str, Dict] = {}\n",
    "    times: Dict[str, float] = {}\n",
    "    predictions_for_stats: Dict[str, Tuple] = {}  # For statistical testing\n",
    "\n",
    "    # ============================================\n",
    "    # Train Baseline Model\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING BASELINE MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    baseline = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device)\n",
    "    crit_base = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=0.0)\n",
    "    opt_base = optim.AdamW(filter(lambda p: p.requires_grad, baseline.parameters()), \n",
    "                           lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    sch_base = optim.lr_scheduler.ReduceLROnPlateau(opt_base, mode='min', factor=0.2, \n",
    "                                                     patience=Config.patience_lr_scheduler)\n",
    "    hist_b, t_b = train_model(baseline, train_loader, val_loader, crit_base, opt_base, sch_base, 'Baseline')\n",
    "    res_b = evaluate_model(baseline, test_loader, 'Baseline')\n",
    "    results['Baseline'] = res_b\n",
    "    times['Baseline'] = t_b\n",
    "\n",
    "    # Collect predictions for statistical testing\n",
    "    y_true_b, y_pred_b, conf_b, prob_b = collect_predictions(baseline, test_loader)\n",
    "    predictions_for_stats['Baseline'] = (y_true_b, y_pred_b, prob_b)\n",
    "\n",
    "    # ============================================\n",
    "    # Train PPDRQ-weighted CE Model\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING PPDRQ-WEIGHTED CE MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    class PPDRQWeightedCE(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super().__init__()\n",
    "            self.num_classes = num_classes\n",
    "        def forward(self, logits, _emb, labels, **kwargs):\n",
    "            pp = compute_ppdrq_from_logits(logits, self.num_classes)\n",
    "            per_ce = F.cross_entropy(logits, labels, reduction='none', label_smoothing=Config.label_smoothing)\n",
    "            w = 1 + (1 - pp)\n",
    "            loss = torch.mean(per_ce * w)\n",
    "            return loss, loss, torch.tensor(0.0, device=logits.device), pp\n",
    "\n",
    "    ppdrq_model = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device)\n",
    "    crit_pp = PPDRQWeightedCE(num_classes=Config.num_classes)\n",
    "    opt_pp = optim.AdamW(filter(lambda p: p.requires_grad, ppdrq_model.parameters()), \n",
    "                         lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    sch_pp = optim.lr_scheduler.ReduceLROnPlateau(opt_pp, mode='min', factor=0.2, \n",
    "                                                   patience=Config.patience_lr_scheduler)\n",
    "    hist_p, t_p = train_model(ppdrq_model, train_loader, val_loader, crit_pp, opt_pp, sch_pp, 'PPDRQ_CE')\n",
    "    res_p = evaluate_model(ppdrq_model, test_loader, 'PPDRQ_CE')\n",
    "    results['PPDRQ_CE'] = res_p\n",
    "    times['PPDRQ_CE'] = t_p\n",
    "\n",
    "    y_true_p, y_pred_p, conf_p, prob_p = collect_predictions(ppdrq_model, test_loader)\n",
    "    predictions_for_stats['PPDRQ_CE'] = (y_true_p, y_pred_p, prob_p)\n",
    "\n",
    "    # ============================================\n",
    "    # Train Triplet Model\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING TRIPLET MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    triplet_model = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device)\n",
    "    crit_trip = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=Config.lambda_triplet)\n",
    "    opt_trip = optim.AdamW(filter(lambda p: p.requires_grad, triplet_model.parameters()), \n",
    "                           lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    sch_trip = optim.lr_scheduler.ReduceLROnPlateau(opt_trip, mode='min', factor=0.2, \n",
    "                                                     patience=Config.patience_lr_scheduler)\n",
    "    hist_t, t_t = train_model(triplet_model, train_loader, val_loader, crit_trip, opt_trip, sch_trip, 'Triplet')\n",
    "    res_t = evaluate_model(triplet_model, test_loader, 'Triplet')\n",
    "    results['Triplet'] = res_t\n",
    "    times['Triplet'] = t_t\n",
    "\n",
    "    y_true_t, y_pred_t, conf_t, prob_t = collect_predictions(triplet_model, test_loader)\n",
    "    predictions_for_stats['Triplet'] = (y_true_t, y_pred_t, prob_t)\n",
    "\n",
    "    # ============================================\n",
    "    # Train and Evaluate MC-Dropout Model (FIXED)\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MC-DROPOUT MODEL (FIXED IMPLEMENTATION)\")\n",
    "    print(\"=\"*80)\n",
    "    mc_model = CustomInceptionV3(num_classes=Config.num_classes, \n",
    "                                 dropout_rate=Config.mc_dropout_rate).to(Config.device)\n",
    "    crit_mc = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=0.0)\n",
    "    opt_mc = optim.AdamW(filter(lambda p: p.requires_grad, mc_model.parameters()), \n",
    "                         lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    sch_mc = optim.lr_scheduler.ReduceLROnPlateau(opt_mc, mode='min', factor=0.2, \n",
    "                                                   patience=Config.patience_lr_scheduler)\n",
    "    hist_m, t_m = train_model(mc_model, train_loader, val_loader, crit_mc, opt_mc, sch_mc, 'MC_Dropout')\n",
    "    \n",
    "    # FIXED: Use proper MC-Dropout evaluation\n",
    "    res_m = mc_dropout_predict(mc_model, test_loader, num_passes=Config.mc_dropout_passes)\n",
    "    results['MC_Dropout'] = res_m\n",
    "    times['MC_Dropout_Train'] = t_m\n",
    "    times['MC_Dropout_Infer'] = res_m['inference_time']\n",
    "\n",
    "    y_true_m, y_pred_m, conf_m, prob_m = collect_predictions_mc_dropout(mc_model, test_loader)\n",
    "    predictions_for_stats['MC_Dropout'] = (y_true_m, y_pred_m, prob_m)\n",
    "\n",
    "    # ============================================\n",
    "    # Train Deep Ensemble\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING DEEP ENSEMBLE\")\n",
    "    print(\"=\"*80)\n",
    "    members = []\n",
    "    t_mem = []\n",
    "    for i in range(Config.num_ensemble_models):\n",
    "        m, t, _ = train_ensemble_member(i, train_loader, val_loader)\n",
    "        members.append(m)\n",
    "        t_mem.append(t)\n",
    "    \n",
    "    res_e = evaluate_deep_ensemble(members, test_loader)\n",
    "    results['Ensemble'] = res_e\n",
    "    times['Ensemble_Train'] = sum(t_mem)\n",
    "    times['Ensemble_Infer'] = res_e['inference_time']\n",
    "\n",
    "    # Collect ensemble predictions\n",
    "    all_prob_e = []\n",
    "    all_y_e = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _, _ in test_loader:\n",
    "            xb = xb.to(Config.device)\n",
    "            mem_probs = []\n",
    "            for m in members:\n",
    "                m.eval()\n",
    "                logits, _, _ = m(xb)\n",
    "                mem_probs.append(torch.softmax(logits, dim=1).unsqueeze(0))\n",
    "            mean_prob = torch.cat(mem_probs, 0).mean(0)\n",
    "            all_prob_e.append(mean_prob.cpu())\n",
    "            all_y_e.append(torch.as_tensor(yb).cpu())\n",
    "    prob_e = torch.cat(all_prob_e, 0)\n",
    "    y_e = torch.cat(all_y_e, 0)\n",
    "    pred_e = prob_e.argmax(1)\n",
    "    predictions_for_stats['Ensemble'] = (y_e.numpy(), pred_e.numpy(), prob_e.numpy())\n",
    "\n",
    "    # ============================================\n",
    "    # Statistical Significance Testing (NEW)\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERFORMING STATISTICAL SIGNIFICANCE TESTING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    stat_df = compute_statistical_comparisons(predictions_for_stats, baseline_name='Ensemble')\n",
    "    \n",
    "    # Plot confidence intervals\n",
    "    for metric in ['Accuracy', 'F1-Score', 'Precision', 'Recall']:\n",
    "        plot_confidence_intervals(stat_df, metric=metric)\n",
    "\n",
    "    # ============================================\n",
    "    # Results Summary\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Model':<15}{'Acc':>8}{'Prec':>8}{'Rec':>8}{'F1':>8}{'PPDRQ':>9}{'ECE':>8}{'Brier':>8}{'Train(s)':>10}{'Infer(s)':>10}\")\n",
    "    print(\"-\"*110)\n",
    "    for k, v in results.items():\n",
    "        tr = times.get(k, times.get(f\"{k}_Train\", 0.0))\n",
    "        inf = v.get('inference_time', times.get(f\"{k}_Infer\", 0.0))\n",
    "        print(f\"{k:<15}{v['accuracy']:>8.3f}{v['precision']:>8.3f}{v['recall']:>8.3f}{v['f1_score']:>8.3f}{v['mean_ppdrq']:>9.3f}{v['ece']:>8.3f}{v['brier_score']:>8.3f}{tr:>10.1f}{inf:>10.1f}\")\n",
    "\n",
    "    # ============================================\n",
    "    # Save Training Curves\n",
    "    # ============================================\n",
    "    for name, hist in [('Baseline', hist_b), ('PPDRQ_CE', hist_p), ('Triplet', hist_t)]:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(hist['train_acc'], label='Train')\n",
    "        plt.plot(hist['val_acc'], label='Val')\n",
    "        plt.title(f'{name} Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(hist['train_loss'], label='Train')\n",
    "        plt.plot(hist['val_loss'], label='Val')\n",
    "        plt.title(f'{name} Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(Config.output_dir, f\"{name}_curves.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # ============================================\n",
    "    # Generate Paper-Ready Outputs\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING PAPER-READY VISUALIZATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Unified results table\n",
    "    df = save_results_table(results, times, \"results_all_models.csv\")\n",
    "    \n",
    "    # Metric comparison plots\n",
    "    plot_metric_bars(df, \"Accuracy\", \"cmp_accuracy.png\")\n",
    "    plot_metric_bars(df, \"F1\", \"cmp_f1.png\")\n",
    "    plot_metric_bars(df, \"ECE\", \"cmp_ece.png\")\n",
    "    plot_metric_bars(df, \"Mean_PPDRQ\", \"cmp_ppdrq.png\")\n",
    "    \n",
    "    # Cost-performance analysis\n",
    "    plot_cost_performance(df, perf_metric=\"Accuracy\", cost_metric=\"Infer_Time_s\", \n",
    "                         fname=\"cost_perf_acc_vs_infer.png\")\n",
    "\n",
    "    # ============================================\n",
    "    # Confusion Matrices and Per-Class Reports\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING CONFUSION MATRICES AND PER-CLASS REPORTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    model_objs = {\n",
    "        'Baseline': baseline,\n",
    "        'PPDRQ_CE': ppdrq_model,\n",
    "        'Triplet': triplet_model,\n",
    "        'MC_Dropout': mc_model\n",
    "    }\n",
    "    \n",
    "    # Ensemble wrapper\n",
    "    class EnsembleWrapper(nn.Module):\n",
    "        def __init__(self, members):\n",
    "            super().__init__()\n",
    "            self.members = members\n",
    "        def forward(self, x):\n",
    "            outs = []\n",
    "            for m in self.members:\n",
    "                m.eval()\n",
    "                lo, _, _ = m(x)\n",
    "                outs.append(lo.unsqueeze(0))\n",
    "            logits = torch.mean(torch.cat(outs, 0), dim=0)\n",
    "            return logits, torch.zeros(x.size(0), 512, device=logits.device), {'final_logits': logits}\n",
    "    \n",
    "    ens_wrapper = EnsembleWrapper(members)\n",
    "    model_objs['Ensemble'] = ens_wrapper\n",
    "\n",
    "    for name, m in model_objs.items():\n",
    "        if name == 'MC_Dropout':\n",
    "            y_t, y_p, confs, prob = collect_predictions_mc_dropout(m, test_loader)\n",
    "        else:\n",
    "            y_t, y_p, confs, prob = collect_predictions(m, test_loader)\n",
    "        \n",
    "        plot_confusion(y_t, y_p, CLASS_NAMES, f\"Confusion Matrix: {name}\", \n",
    "                      f\"cm_{name}.png\", normalize=True)\n",
    "        export_per_class_report(y_t, y_p, CLASS_NAMES, f\"per_class_{name}.csv\")\n",
    "\n",
    "    # ============================================\n",
    "    # Reliability Grid\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING RELIABILITY GRID\")\n",
    "    print(\"=\"*80)\n",
    "    reliability_grid(model_objs, test_loader, fname='reliability_grid_test.png', num_bins=10)\n",
    "\n",
    "    # ============================================\n",
    "    # Layer-wise PPDRQ Analysis\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING LAYER-WISE PPDRQ ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    get_and_plot_layerwise_ppdrq(ppdrq_model, test_loader, 'PPDRQ_CE', 'PPDRQ Model')\n",
    "\n",
    "    # ============================================\n",
    "    # Clinical Validation Framework (NEW)\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CLINICAL VALIDATION FRAMEWORK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Generate clinical validation reports for key models\n",
    "    for model_name, model in [('PPDRQ_CE', ppdrq_model), ('Ensemble', ens_wrapper)]:\n",
    "        clinical_validator = ClinicalValidationFramework(model, test_loader, model_name)\n",
    "        selected_cases = clinical_validator.select_cases_for_review(\n",
    "            n_high_conf=10, n_low_conf=10, n_errors=10\n",
    "        )\n",
    "        clinical_validator.generate_validation_report(\n",
    "            output_file=f'clinical_validation_{model_name}.csv'\n",
    "        )\n",
    "    \n",
    "    # Print template for neurologist\n",
    "    clinical_validator.print_example_observations()\n",
    "\n",
    "    # ============================================\n",
    "    # Domain Shift Evaluation (Unseen Dataset)\n",
    "    # ============================================\n",
    "    if len(unseen_dataset) > 0:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DOMAIN SHIFT EVALUATION (UNSEEN DATASET)\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"Note: DermNet or other domain provides cross-domain evaluation.\")\n",
    "        print(\"Limitation: Substantial domain differences from MRI neuroimaging.\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Load best weights\n",
    "        for tag, model in [('Baseline', baseline), ('PPDRQ_CE', ppdrq_model), \n",
    "                          ('Triplet', triplet_model), ('MC_Dropout', mc_model)]:\n",
    "            pth = os.path.join(Config.output_dir, f\"{tag}_best.pth\")\n",
    "            if os.path.exists(pth):\n",
    "                model.load_state_dict(torch.load(pth, map_location=Config.device))\n",
    "        \n",
    "        # Evaluate on unseen\n",
    "        res_b_u = evaluate_model(baseline, unseen_loader, 'Baseline_Unseen')\n",
    "        res_p_u = evaluate_model(ppdrq_model, unseen_loader, 'PPDRQ_CE_Unseen')\n",
    "        res_t_u = evaluate_model(triplet_model, unseen_loader, 'Triplet_Unseen')\n",
    "        res_m_u = mc_dropout_predict(mc_model, unseen_loader, num_passes=Config.mc_dropout_passes)\n",
    "        \n",
    "        if len(members) == Config.num_ensemble_models:\n",
    "            res_e_u = evaluate_deep_ensemble(members, unseen_loader)\n",
    "        \n",
    "        # Save unseen results\n",
    "        unseen_results = {\n",
    "            'Baseline_Unseen': res_b_u,\n",
    "            'PPDRQ_CE_Unseen': res_p_u,\n",
    "            'Triplet_Unseen': res_t_u,\n",
    "            'MC_Dropout_Unseen': res_m_u,\n",
    "        }\n",
    "        if len(members) == Config.num_ensemble_models:\n",
    "            unseen_results['Ensemble_Unseen'] = res_e_u\n",
    "        \n",
    "        df_unseen = pd.DataFrame([\n",
    "            {'Model': k, **v} for k, v in unseen_results.items()\n",
    "        ])\n",
    "        df_unseen.to_csv(os.path.join(Config.output_dir, 'results_unseen_domain.csv'), index=False)\n",
    "        print(\"\\n✓ Saved unseen domain results\")\n",
    "        \n",
    "        print(\"\\n⚠️  LIMITATION ACKNOWLEDGMENT:\")\n",
    "        print(\"The unseen dataset evaluation (e.g., DermNet) provides useful cross-domain\")\n",
    "        print(\"robustness testing but differs substantially from MRI neuroimaging in:\")\n",
    "        print(\"  - Imaging modality (dermoscopic vs MRI)\")\n",
    "        print(\"  - Anatomical structures (skin vs brain)\")\n",
    "        print(\"  - Pathology types (dermatological vs neurodegenerative)\")\n",
    "        print(\"\\nFor clinical deployment, site-held-out or scanner-specific MRI robustness\")\n",
    "        print(\"testing would provide more clinically relevant OOD evaluation.\")\n",
    "\n",
    "    # ============================================\n",
    "    # Final Summary Document\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATING FINAL SUMMARY DOCUMENT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "PPDRQ RELIABILITY ENHANCEMENT - EXPERIMENTAL RESULTS SUMMARY\n",
    "{'='*80}\n",
    "\n",
    "1. DATASET VERIFICATION\n",
    "   - Patient-level split verification completed\n",
    "   - See: patient_split_verification.csv\n",
    "   \n",
    "2. MODEL PERFORMANCE\n",
    "   All models evaluated on test set with comprehensive metrics:\n",
    "   - See: results_all_models.csv\n",
    "   \n",
    "3. STATISTICAL SIGNIFICANCE\n",
    "   Bootstrap confidence intervals and McNemar tests computed:\n",
    "   - See: statistical_significance_results.csv\n",
    "   - Visualizations: *_confidence_intervals.png\n",
    "   \n",
    "4. CALIBRATION ANALYSIS\n",
    "   - Reliability diagrams for all models\n",
    "   - ECE and Brier scores computed\n",
    "   - See: reliability_grid_test.png\n",
    "   \n",
    "5. PER-CLASS PERFORMANCE\n",
    "   - Confusion matrices: cm_*.png\n",
    "   - Detailed reports: per_class_*.csv\n",
    "   \n",
    "6. CLINICAL VALIDATION\n",
    "   - Cases selected for neurologist review\n",
    "   - See: clinical_validation_*.csv\n",
    "   - Template provided for clinical observations\n",
    "   \n",
    "7. DOMAIN SHIFT EVALUATION\n",
    "   {'- Completed on unseen dataset' if len(unseen_dataset) > 0 else '- Not performed (no unseen data)'}\n",
    "   {'- See: results_unseen_domain.csv' if len(unseen_dataset) > 0 else ''}\n",
    "   {'- Limitation acknowledged in results' if len(unseen_dataset) > 0 else ''}\n",
    "\n",
    "8. KEY FINDINGS\n",
    "   Best performing model: {max(results.keys(), key=lambda k: results[k]['accuracy'])}\n",
    "   Accuracy: {max(results.values(), key=lambda v: v['accuracy'])['accuracy']:.4f}\n",
    "   \n",
    "   Statistical significance vs Baseline:\n",
    "   {stat_df[stat_df['Significant_vs_Baseline'] == True]['Model'].tolist()}\n",
    "\n",
    "{'='*80}\n",
    "\n",
    "All results saved to: {Config.output_dir}/\n",
    "\n",
    "REVIEWER REQUIREMENTS ADDRESSED:\n",
    "✓ 1. MC-Dropout implementation fixed and verified\n",
    "✓ 2. Statistical significance testing completed\n",
    "✓ 3. Patient-level split verification documented\n",
    "✓ 4. Clinical validation framework implemented\n",
    "✓ 5. Domain shift limitation acknowledged\n",
    "\"\"\"\n",
    "\n",
    "    with open(os.path.join(Config.output_dir, 'EXPERIMENTAL_SUMMARY.txt'), 'w') as f:\n",
    "        f.write(summary_text)\n",
    "    \n",
    "    print(summary_text)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✓ ALL EXPERIMENTS COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nAll results, visualizations, and reports saved to: {Config.output_dir}/\")\n",
    "    print(\"\\nKey files for paper:\")\n",
    "    print(\"  - results_all_models.csv\")\n",
    "    print(\"  - statistical_significance_results.csv\")\n",
    "    print(\"  - patient_split_verification.csv\")\n",
    "    print(\"  - clinical_validation_*.csv\")\n",
    "    print(\"  - All visualization PNG files\")\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f95e68d",
   "metadata": {},
   "source": [
    "# PPDRQ\n",
    "ENHANCED ANALYSIS, λ sensitivity analysis with multiple metrics, Detailed timing analysis, Comprehensive methodological transparency report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d4039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set professional plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Professional color schemes\n",
    "COLORS = {\n",
    "    'primary': '#2E86AB',\n",
    "    'secondary': '#A23B72', \n",
    "    'accent': '#F18F01',\n",
    "    'success': '#C73E1D',\n",
    "    'info': '#7209B7',\n",
    "    'warning': '#F4A261',\n",
    "    'light': '#E9C46A',\n",
    "    'dark': '#264653'\n",
    "}\n",
    "\n",
    "MODEL_COLORS = {\n",
    "    'Baseline': COLORS['primary'],\n",
    "    'PPDRQ_CE': COLORS['secondary'],\n",
    "    'Triplet': COLORS['accent'],\n",
    "    'MC_Dropout': COLORS['success'],\n",
    "    'Ensemble': COLORS['info']\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Enhanced Config with Methodological Transparency\n",
    "# -------------------------------\n",
    "class Config:\n",
    "    data_dir = 'dataset'              \n",
    "    output_dir = '.Save results here'\n",
    "\n",
    "    image_size = 299\n",
    "    num_classes = 3                \n",
    "\n",
    "    batch_size = 16                \n",
    "    epochs = 60                    \n",
    "    learning_rate = 3e-4\n",
    "    weight_decay = 5e-4            \n",
    "    patience_lr_scheduler = 3\n",
    "    patience_early_stopping = 10\n",
    "    grad_clip = 1.0\n",
    "\n",
    "    # METHODOLOGICAL TRANSPARENCY: PPDRQ Parameters\n",
    "    lambda_triplet = 0.1           # λ: Triplet loss weight\n",
    "    triplet_margin = 0.2\n",
    "    epsilon_p = 0.1                # εp: Positive sample threshold  \n",
    "    epsilon_n = 0.2                # εn: Negative sample threshold\n",
    "\n",
    "    # MC-Dropout & Ensemble\n",
    "    mc_dropout_passes = 30\n",
    "    num_ensemble_models = 5\n",
    "\n",
    "    # Regularization toggles\n",
    "    use_mixup_cutmix = True\n",
    "    mixup_alpha = 0.2\n",
    "    cutmix_alpha = 0.2\n",
    "    label_smoothing = 0.05\n",
    "\n",
    "    # Data safety\n",
    "    allow_make_dummy = False       \n",
    "\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Professional plotting settings\n",
    "    FIGURE_DPI = 300\n",
    "    FONT_SIZE = 12\n",
    "    TITLE_SIZE = 14\n",
    "    LABEL_SIZE = 10\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def setup_professional_plots():\n",
    "    \"\"\"Configure matplotlib for professional publication-ready plots\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'figure.dpi': Config.FIGURE_DPI,\n",
    "        'savefig.dpi': Config.FIGURE_DPI,\n",
    "        'font.size': Config.FONT_SIZE,\n",
    "        'axes.titlesize': Config.TITLE_SIZE,\n",
    "        'axes.labelsize': Config.LABEL_SIZE,\n",
    "        'xtick.labelsize': Config.LABEL_SIZE,\n",
    "        'ytick.labelsize': Config.LABEL_SIZE,\n",
    "        'legend.fontsize': Config.LABEL_SIZE,\n",
    "        'figure.titlesize': Config.TITLE_SIZE,\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3,\n",
    "        'axes.axisbelow': True,\n",
    "        'savefig.bbox': 'tight',\n",
    "        'savefig.pad_inches': 0.1\n",
    "    })\n",
    "\n",
    "\n",
    "os.makedirs(Config.output_dir, exist_ok=True)\n",
    "set_seed(42)\n",
    "setup_professional_plots()\n",
    "print(f\"Using device: {Config.device}\")\n",
    "\n",
    "# Print methodological transparency info\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METHODOLOGICAL TRANSPARENCY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PPDRQ Parameters:\")\n",
    "print(f\"  λ (lambda_triplet): {Config.lambda_triplet}\")\n",
    "print(f\"  εp (epsilon_p): {Config.epsilon_p}\")\n",
    "print(f\"  εn (epsilon_n): {Config.epsilon_n}\")\n",
    "print(f\"  Triplet margin: {Config.triplet_margin}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Dataset (unchanged core logic)\n",
    "# -------------------------------\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, phase: str, transform=None):\n",
    "        self.data_dir = os.path.join(data_dir, phase)\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {'AD': 0, 'MCI': 1, 'NC': 2}\n",
    "        self.image_paths: List[str] = []\n",
    "        self.labels: List[int] = []\n",
    "\n",
    "        for cname, idx in self.class_to_idx.items():\n",
    "            cdir = os.path.join(self.data_dir, cname)\n",
    "            if not os.path.exists(cdir):\n",
    "                if Config.allow_make_dummy:\n",
    "                    print(f\"[WARN] Missing {cdir}. Creating dummy images for demo only.\")\n",
    "                    os.makedirs(cdir, exist_ok=True)\n",
    "                    n = 8 if phase != 'train' else 64\n",
    "                    for i in range(n):\n",
    "                        img = Image.new('L', (Config.image_size, Config.image_size), color=random.randint(0, 255))\n",
    "                        img.convert('RGB').save(os.path.join(cdir, f'dummy_{i}.png'))\n",
    "                else:\n",
    "                    print(f\"[WARN] Missing class dir: {cdir}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "            files = [f for f in os.listdir(cdir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            if not files and Config.allow_make_dummy:\n",
    "                print(f\"[WARN] Empty {cdir}. Creating dummy images for demo only.\")\n",
    "                n = 8 if phase != 'train' else 64\n",
    "                for i in range(n):\n",
    "                    img = Image.new('L', (Config.image_size, Config.image_size), color=random.randint(0, 255))\n",
    "                    img.convert('RGB').save(os.path.join(cdir, f'dummy_{i}.png'))\n",
    "                files = [f for f in os.listdir(cdir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "            for name in files:\n",
    "                self.image_paths.append(os.path.join(cdir, name))\n",
    "                self.labels.append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        p = self.image_paths[idx]\n",
    "        y = self.labels[idx]\n",
    "        img = Image.open(p).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, y, idx\n",
    "\n",
    "# Data transforms (unchanged)\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize(int(Config.image_size*1.1)),\n",
    "    transforms.RandomResizedCrop(Config.image_size, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3), value='random')\n",
    "])\n",
    "\n",
    "val_test_tfms = transforms.Compose([\n",
    "    transforms.Resize((Config.image_size, Config.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = MRIDataset(Config.data_dir, 'train', train_tfms)\n",
    "val_dataset   = MRIDataset(Config.data_dir, 'validation', val_test_tfms)\n",
    "test_dataset  = MRIDataset(Config.data_dir, 'test', val_test_tfms)\n",
    "unseen_dataset= MRIDataset(Config.data_dir, 'unseen', val_test_tfms)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}, Unseen: {len(unseen_dataset)}\")\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"[ERROR] Train dataset is empty. Please prepare data under data/train/{AD,MCI,NC}.\")\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=0)\n",
    "unseen_loader= DataLoader(unseen_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Model (unchanged)\n",
    "# -------------------------------\n",
    "class CustomInceptionV3(nn.Module):\n",
    "    def __init__(self, num_classes=3, dropout_rate=0.6, include_aux_logits=True, train_last_blocks_only=True):\n",
    "        super().__init__()\n",
    "        import torchvision\n",
    "        try:\n",
    "            self.inception_base = torchvision.models.inception_v3(\n",
    "                weights=torchvision.models.Inception_V3_Weights.IMAGENET1K_V1,\n",
    "                aux_logits=True\n",
    "            )\n",
    "        except Exception:\n",
    "            self.inception_base = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "        self.inception_base.fc = nn.Identity()\n",
    "\n",
    "        if train_last_blocks_only:\n",
    "            for p in self.inception_base.parameters():\n",
    "                p.requires_grad = False\n",
    "            for name, m in self.inception_base.named_modules():\n",
    "                if name.startswith('Mixed_7'):\n",
    "                    for p in m.parameters():\n",
    "                        p.requires_grad = True\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(2048)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.hidden1 = nn.Linear(2048, 1024)\n",
    "        self.hidden2 = nn.Linear(1024, 512)\n",
    "        self.fc_head = nn.Linear(2048, num_classes)\n",
    "\n",
    "        self.mixed_7c_output = None\n",
    "        def hook_fn(_m, _in, out):\n",
    "            self.mixed_7c_output = out\n",
    "        self.inception_base.Mixed_7c.register_forward_hook(hook_fn)\n",
    "\n",
    "    def _extract_features(self, x):\n",
    "        out = self.inception_base(x)\n",
    "        if hasattr(out, 'logits'):\n",
    "            return out.logits\n",
    "        if isinstance(out, (tuple, list)) and len(out) > 0:\n",
    "            return out[0]\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        flatten = self._extract_features(x)\n",
    "        z = self.bn(flatten)\n",
    "        z = self.dropout(z)\n",
    "        logits = self.fc_head(z)\n",
    "        e = F.relu(self.hidden1(z))\n",
    "        e = F.relu(self.hidden2(e))\n",
    "        layer_dict = {\n",
    "            'base_model_output': self.mixed_7c_output,\n",
    "            'hidden_layer1': F.relu(self.hidden1(self.bn(flatten))),\n",
    "            'hidden_layer2': e,\n",
    "            'flatten': flatten,\n",
    "            'final_logits': logits\n",
    "        }\n",
    "        return logits, e, layer_dict\n",
    "\n",
    "# -------------------------------\n",
    "# 4. PPDRQ & Losses (unchanged core logic)\n",
    "# -------------------------------\n",
    "def compute_ppdrq_from_logits(logits: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    if num_classes == 3:\n",
    "        p1, p2, p3 = probs[:, 0], probs[:, 1], probs[:, 2]\n",
    "        d12 = (p1 - p2).abs(); d13 = (p1 - p3).abs(); d23 = (p2 - p3).abs()\n",
    "        raw = (d12 + d13 + d23) / 3.0\n",
    "        pp = (3/2) * raw\n",
    "        return torch.clamp(pp, 0, 1)\n",
    "    diffs = []\n",
    "    for i in range(num_classes):\n",
    "        for j in range(i+1, num_classes):\n",
    "            diffs.append((probs[:, i] - probs[:, j]).abs())\n",
    "    sumdiff = torch.stack(diffs, dim=1).sum(dim=1)\n",
    "    pp = sumdiff / (num_classes - 1)\n",
    "    return torch.clamp(pp, 0, 1)\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, num_classes, lambda_triplet=Config.lambda_triplet, margin=Config.triplet_margin,\n",
    "                 epsilon_p=Config.epsilon_p, epsilon_n=Config.epsilon_n, smoothing=Config.label_smoothing):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_triplet = lambda_triplet\n",
    "        self.epsilon_p = epsilon_p\n",
    "        self.epsilon_n = epsilon_n\n",
    "        self.triplet = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, logits, embeddings, labels,\n",
    "                all_feats=None, all_labels=None, all_pp=None):\n",
    "        pp = compute_ppdrq_from_logits(logits, self.num_classes)\n",
    "        per_sample_ce = F.cross_entropy(logits, labels, reduction='none', label_smoothing=self.smoothing)\n",
    "        weights = 1 + (1 - pp)\n",
    "        ce_loss = torch.mean(per_sample_ce * weights)\n",
    "\n",
    "        triplet_loss = torch.tensor(0.0, device=logits.device)\n",
    "        if all_feats is not None and len(all_feats) > 0:\n",
    "            allF = torch.cat(all_feats, dim=0).to(embeddings.device)\n",
    "            allY = torch.cat(all_labels, dim=0).to(embeddings.device)\n",
    "            allP = torch.cat(all_pp, dim=0).to(embeddings.device)\n",
    "            count = 0\n",
    "            for i in range(labels.size(0)):\n",
    "                a = embeddings[i]\n",
    "                y = labels[i]\n",
    "                ppa = pp[i]\n",
    "                pos_idx = torch.where((allY == y) & ((allP - ppa).abs() < Config.epsilon_p))[0]\n",
    "                neg_idx = torch.where((allY != y) & ((allP - ppa).abs() >= Config.epsilon_n))[0]\n",
    "                if pos_idx.numel() > 1 and neg_idx.numel() > 0:\n",
    "                    p = allF[random.choice(pos_idx.tolist())]\n",
    "                    n = allF[random.choice(neg_idx.tolist())]\n",
    "                    triplet_loss = triplet_loss + self.triplet(a.unsqueeze(0), p.unsqueeze(0), n.unsqueeze(0))\n",
    "                    count += 1\n",
    "            if count > 0:\n",
    "                triplet_loss = triplet_loss / count\n",
    "        total = ce_loss + self.lambda_triplet * triplet_loss\n",
    "        return total, ce_loss, triplet_loss, pp\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Mixup/CutMix utils (unchanged)\n",
    "# -------------------------------\n",
    "def rand_bbox(W, H, lam):\n",
    "    cut_rat = math.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    x1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    y1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    x2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    y2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def apply_mixup_cutmix(x, y):\n",
    "    if not Config.use_mixup_cutmix:\n",
    "        return x, y, None\n",
    "    r = random.random()\n",
    "    if r < 0.5:\n",
    "        lam = np.random.beta(Config.mixup_alpha, Config.mixup_alpha)\n",
    "        idx = torch.randperm(x.size(0)).to(x.device)\n",
    "        mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "        y_a, y_b = y, y[idx]\n",
    "        return mixed_x, (y_a, y_b, lam), 'mixup'\n",
    "    else:\n",
    "        lam = np.random.beta(Config.cutmix_alpha, Config.cutmix_alpha)\n",
    "        idx = torch.randperm(x.size(0)).to(x.device)\n",
    "        x1, y1, x2, y2 = rand_bbox(x.size(3), x.size(2), lam)\n",
    "        x_mix = x.clone()\n",
    "        x_mix[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]\n",
    "        lam = 1 - ((x2 - x1) * (y2 - y1) / (x.size(-1) * x.size(-2)))\n",
    "        y_a, y_b = y, y[idx]\n",
    "        return x_mix, (y_a, y_b, lam), 'cutmix'\n",
    "\n",
    "def mix_criterion(logits, target_tuple):\n",
    "    y_a, y_b, lam = target_tuple\n",
    "    loss_a = F.cross_entropy(logits, y_a, label_smoothing=Config.label_smoothing)\n",
    "    loss_b = F.cross_entropy(logits, y_b, label_smoothing=Config.label_smoothing)\n",
    "    return lam * loss_a + (1 - lam) * loss_b\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Enhanced Training/Evaluation with Professional Plots\n",
    "# -------------------------------\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, model_name, num_epochs=Config.epochs):\n",
    "    best_val = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'train_ppdrq': [], 'val_ppdrq': [], 'epochs_trained': 0}\n",
    "    start = time.time()\n",
    "\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        t_loss = 0.0; t_correct = 0; t_total = 0; t_pp = 0.0\n",
    "\n",
    "        # Pre-collect for triplet\n",
    "        allF, allY, allP = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _ in train_loader:\n",
    "                xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device)\n",
    "                logits, emb, _ = model(xb)\n",
    "                pp = compute_ppdrq_from_logits(logits, Config.num_classes)\n",
    "                allF.append(emb.detach().cpu()); allY.append(yb.detach().cpu()); allP.append(pp.detach().cpu())\n",
    "        allF = torch.cat(allF, 0) if allF else torch.tensor([])\n",
    "        allY = torch.cat(allY, 0) if allY else torch.tensor([])\n",
    "        allP = torch.cat(allP, 0) if allP else torch.tensor([])\n",
    "\n",
    "        for xb, yb, _ in train_loader:\n",
    "            xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device)\n",
    "            xb_m, yb_m, aug = apply_mixup_cutmix(xb, yb)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits, emb, _ = model(xb_m)\n",
    "\n",
    "            if aug is None:\n",
    "                total, ce, trip, pp = criterion(logits, emb, yb, [allF], [allY], [allP])\n",
    "            else:\n",
    "                ce_mix = mix_criterion(logits, yb_m)\n",
    "                pp = compute_ppdrq_from_logits(logits, Config.num_classes)\n",
    "                total = ce_mix\n",
    "                ce, trip = ce_mix, torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "            total.backward()\n",
    "            if Config.grad_clip is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), Config.grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                pred = probs.argmax(1)\n",
    "                if aug is None:\n",
    "                    t_correct += (pred == yb).sum().item()\n",
    "                    t_total += yb.size(0)\n",
    "                else:\n",
    "                    y_a, _, lam = yb_m\n",
    "                    t_correct += (pred == y_a).sum().item() * lam\n",
    "                    t_total += y_a.size(0)\n",
    "                t_loss += total.item() * xb.size(0)\n",
    "                t_pp += pp.sum().item()\n",
    "\n",
    "        tr_loss = t_loss / max(t_total, 1)\n",
    "        tr_acc = t_correct / max(t_total, 1)\n",
    "        tr_pp = t_pp / max(t_total, 1)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        v_loss=0.0; v_cor=0; v_tot=0; v_pp=0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _ in val_loader:\n",
    "                xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device)\n",
    "                logits, emb, _ = model(xb)\n",
    "                total, ce, trip, pp = criterion(logits, emb, yb)\n",
    "                probs = torch.softmax(logits, dim=1)\n",
    "                pred = probs.argmax(1)\n",
    "                v_cor += (pred == yb).sum().item()\n",
    "                v_tot += yb.size(0)\n",
    "                v_loss += total.item() * xb.size(0)\n",
    "                v_pp += pp.sum().item()\n",
    "        va_loss = v_loss / max(v_tot, 1)\n",
    "        va_acc  = v_cor / max(v_tot, 1)\n",
    "        va_pp   = v_pp / max(v_tot, 1)\n",
    "\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['val_loss'].append(va_loss)\n",
    "        history['train_acc'].append(tr_acc)\n",
    "        history['val_acc'].append(va_acc)\n",
    "        history['train_ppdrq'].append(tr_pp)\n",
    "        history['val_ppdrq'].append(va_pp)\n",
    "        history['epochs_trained'] = epoch + 1\n",
    "\n",
    "        if (epoch+1) == 1 or (epoch+1) % 5 == 0 or (epoch+1) == num_epochs:\n",
    "            print(f\"Epoch {epoch+1:03d}/{num_epochs} | Train: loss {tr_loss:.4f} acc {tr_acc:.4f} pp {tr_pp:.3f} | Val: loss {va_loss:.4f} acc {va_acc:.4f} pp {va_pp:.3f}\")\n",
    "\n",
    "        scheduler.step(va_loss)\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), os.path.join(Config.output_dir, f\"{model_name}_best.pth\"))\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= Config.patience_early_stopping:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    dur = time.time() - start\n",
    "    print(f\"Training time for {model_name}: {dur:.1f}s\")\n",
    "    return history, dur\n",
    "\n",
    "def calculate_calibration_metrics(probabilities: torch.Tensor, labels: torch.Tensor, num_bins=10):\n",
    "    bins = torch.linspace(0, 1, num_bins + 1)\n",
    "    conf, pred = probabilities.max(1)\n",
    "    acc = (pred == labels).float()\n",
    "    ece = 0.0\n",
    "    for i in range(num_bins):\n",
    "        in_bin = (conf > bins[i]) & (conf <= bins[i+1])\n",
    "        if in_bin.any():\n",
    "            ece += torch.abs(acc[in_bin].mean() - conf[in_bin].mean()) * in_bin.float().mean()\n",
    "    one_hot = F.one_hot(labels, num_classes=Config.num_classes).float()\n",
    "    brier = torch.mean(torch.sum((probabilities - one_hot) ** 2, dim=1))\n",
    "    return ece.item(), brier.item()\n",
    "\n",
    "def plot_reliability_diagram(conf, correct, ece, model_name, num_bins=10):\n",
    "    \"\"\"Enhanced reliability diagram with professional styling\"\"\"\n",
    "    bins = np.linspace(0, 1, num_bins + 1)\n",
    "    mids = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_acc = []\n",
    "    counts = []\n",
    "    \n",
    "    for i in range(num_bins):\n",
    "        m = (conf >= bins[i]) & (conf <= bins[i+1])\n",
    "        if m.sum() > 0:\n",
    "            bin_acc.append(correct[m].float().mean().item())\n",
    "            counts.append(int(m.sum()))\n",
    "        else:\n",
    "            bin_acc.append(0.0); counts.append(0)\n",
    "    \n",
    "    # Create professional plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    ax.plot([0,1], [0,1], 'k--', linewidth=2, alpha=0.7, label='Perfect Calibration')\n",
    "    \n",
    "    # Calibration bars with model-specific color\n",
    "    color = MODEL_COLORS.get(model_name, COLORS['primary'])\n",
    "    bars = ax.bar(mids, bin_acc, width=1/num_bins*0.8, alpha=0.8, \n",
    "                  color=color, edgecolor='white', linewidth=1.5)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        if count > 0:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.02, \n",
    "                   str(count), ha='center', va='bottom', fontweight='bold',\n",
    "                   fontsize=10)\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_xlabel('Confidence', fontweight='bold')\n",
    "    ax.set_ylabel('Accuracy', fontweight='bold')\n",
    "    ax.set_title(f'Reliability Diagram: {model_name}\\nECE = {ece:.3f}', \n",
    "                fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save with high quality\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.output_dir, f\"{model_name}_reliability.png\"), \n",
    "                dpi=Config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, loader, model_name):\n",
    "    model.eval()\n",
    "    all_prob=[]; all_y=[]\n",
    "    t0=time.time()\n",
    "    if len(loader.dataset)==0:\n",
    "        print(f\"[WARN] {model_name} loader empty.\")\n",
    "        return {k:0.0 for k in ['accuracy','precision','recall','f1_score','mean_ppdrq','ece','brier_score','inference_time']}\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _ in tqdm(loader, desc=f\"Evaluating {model_name}\"):\n",
    "            xb=xb.to(Config.device); yb=torch.as_tensor(yb, device=Config.device)\n",
    "            logits, _, _ = model(xb)\n",
    "            prob = torch.softmax(logits, dim=1)\n",
    "            all_prob.append(prob.cpu()); all_y.append(yb.cpu())\n",
    "    t1=time.time()\n",
    "    prob=torch.cat(all_prob,0)\n",
    "    y=torch.cat(all_y,0)\n",
    "    pred=prob.argmax(1)\n",
    "    acc=accuracy_score(y.numpy(), pred.numpy())\n",
    "    prec=precision_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    rec=recall_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    f1=f1_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    pp = compute_ppdrq_from_logits(torch.log(prob+1e-8), Config.num_classes)\n",
    "    ece,brier=calculate_calibration_metrics(prob, y)\n",
    "    plot_reliability_diagram(prob.max(1)[0], (pred==y), ece, model_name)\n",
    "    print(f\"\\n[{model_name}] Acc {acc:.4f} Prec {prec:.4f} Rec {rec:.4f} F1 {f1:.4f} PPDRQ {pp.mean().item():.4f} ECE {ece:.4f} Brier {brier:.4f}\")\n",
    "    return {\n",
    "        'accuracy':acc,'precision':prec,'recall':rec,'f1_score':f1,\n",
    "        'mean_ppdrq':pp.mean().item(),'ece':ece,'brier_score':brier,\n",
    "        'inference_time':t1-t0\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Enhanced Layer-wise PPDRQ Analysis\n",
    "# -------------------------------\n",
    "def get_and_plot_layerwise_ppdrq(model, loader, model_name, title_suffix=\"\"):\n",
    "    \"\"\"Enhanced layer-wise PPDRQ analysis with professional visualization\"\"\"\n",
    "    model.eval()\n",
    "    collect = {k:[] for k in ['base_model_output','hidden_layer1','hidden_layer2','flatten','final_logits']}\n",
    "    if len(loader.dataset)==0:\n",
    "        print(f\"[WARN] No data for layer-wise PPDRQ.\")\n",
    "        return\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, _, _ in loader:\n",
    "            xb=xb.to(Config.device)\n",
    "            logits, _, d = model(xb)\n",
    "            for k,v in d.items():\n",
    "                if k=='base_model_output':\n",
    "                    collect[k].append(v.mean(dim=(2,3)).cpu())\n",
    "                else:\n",
    "                    collect[k].append(v.cpu())\n",
    "    \n",
    "    # Temporary classifiers for each layer\n",
    "    temp_base = nn.Linear(2048, Config.num_classes)\n",
    "    temp_h1   = nn.Linear(1024, Config.num_classes)\n",
    "    temp_h2   = nn.Linear(512, Config.num_classes)\n",
    "    temp_flat = nn.Linear(2048, Config.num_classes)\n",
    "    \n",
    "    mean_vals={}\n",
    "    for k, arr in collect.items():\n",
    "        if not arr: mean_vals[k]=0.0; continue\n",
    "        X = torch.cat(arr,0)\n",
    "        if k=='base_model_output':\n",
    "            l = temp_base(X)\n",
    "        elif k=='hidden_layer1':\n",
    "            l = temp_h1(X)\n",
    "        elif k=='hidden_layer2':\n",
    "            l = temp_h2(X)\n",
    "        elif k=='flatten':\n",
    "            l = temp_flat(X)\n",
    "        else: # final_logits\n",
    "            l = X\n",
    "        pp = compute_ppdrq_from_logits(l, Config.num_classes)\n",
    "        mean_vals[k]=pp.mean().item()\n",
    "    \n",
    "    # Create professional bar plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Normalize values for better visualization\n",
    "    mmax=max(mean_vals.values()) if mean_vals else 1.0\n",
    "    norm={k:(v/mmax if mmax>0 else 0.0) for k,v in mean_vals.items()}\n",
    "    \n",
    "    layers=list(norm.keys())\n",
    "    vals=[norm[k] for k in layers]\n",
    "    \n",
    "    # Create gradient colors\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(layers)))\n",
    "    \n",
    "    bars = ax.bar(range(len(layers)), vals, color=colors, alpha=0.8, \n",
    "                  edgecolor='white', linewidth=2)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, vals)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02, \n",
    "               f\"{val:.3f}\", ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xticks(range(len(layers)))\n",
    "    ax.set_xticklabels([l.replace('_', ' ').title() for l in layers], rotation=45, ha='right')\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_ylabel('Normalized Mean PPDRQ', fontweight='bold')\n",
    "    ax.set_title(f'Layer-wise PPDRQ Analysis: {model_name} {title_suffix}', \n",
    "                fontweight='bold', pad=20)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.output_dir, f\"{model_name}_layer_ppdrq.png\"), \n",
    "                dpi=Config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------------\n",
    "# 8. MC-Dropout & Ensemble (unchanged core logic)\n",
    "# -------------------------------\n",
    "def _set_dropout_mode_only(model: nn.Module, training: bool = True):\n",
    "    \"\"\"Toggle ONLY Dropout layers' mode; keep everything else (e.g., BatchNorm) unchanged.\"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Dropout, nn.Dropout2d, nn.Dropout3d, nn.AlphaDropout)):\n",
    "            m.train(training)\n",
    "\n",
    "def mc_dropout_predict(model, loader, num_passes=Config.mc_dropout_passes):\n",
    "    # --- FIX: stochastic inference protocol ---\n",
    "    # Keep BN frozen by staying in eval(); enable randomness only in Dropout.\n",
    "    model.eval()\n",
    "    _set_dropout_mode_only(model, True)\n",
    "\n",
    "    all_prob=[]; all_y=[]; t0=time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _ in tqdm(loader, desc=f\"MC-Dropout {num_passes} passes\"):\n",
    "            xb=xb.to(Config.device); yb=torch.as_tensor(yb, device=Config.device)\n",
    "            batch = []\n",
    "            for _ in range(num_passes):\n",
    "                logits, _, _ = model(xb)\n",
    "                batch.append(torch.softmax(logits, dim=1).unsqueeze(0))\n",
    "            mean_prob = torch.cat(batch,0).mean(0)\n",
    "            all_prob.append(mean_prob.cpu()); all_y.append(yb.cpu())\n",
    "    t1=time.time()\n",
    "\n",
    "    # restore Dropout layers to eval mode\n",
    "    _set_dropout_mode_only(model, False)\n",
    "\n",
    "    prob=torch.cat(all_prob,0); y=torch.cat(all_y,0)\n",
    "    pred=prob.argmax(1)\n",
    "    acc=accuracy_score(y.numpy(), pred.numpy())\n",
    "    prec=precision_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    rec=recall_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    f1=f1_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    ece,brier=calculate_calibration_metrics(prob, y)\n",
    "    plot_reliability_diagram(prob.max(1)[0], (pred==y), ece, 'MC_Dropout')\n",
    "    pp = compute_ppdrq_from_logits(torch.log(prob+1e-8), Config.num_classes)\n",
    "    print(f\"\\n[MC-Dropout] Acc {acc:.4f} F1 {f1:.4f} PPDRQ {pp.mean().item():.4f} ECE {ece:.4f}\")\n",
    "    return {'accuracy':acc,'precision':prec,'recall':rec,'f1_score':f1,\n",
    "            'mean_ppdrq':pp.mean().item(),'ece':ece,'brier_score':brier,\n",
    "            'inference_time':t1-t0}\n",
    "\n",
    "def train_ensemble_member(i, train_loader, val_loader):\n",
    "    print(f\"\\n--- Training Ensemble Member {i+1} ---\")\n",
    "    m = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device)\n",
    "    crit = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=0.0)  # CE only with smoothing\n",
    "    opt = optim.AdamW(filter(lambda p: p.requires_grad, m.parameters()), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    sch = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.2, patience=Config.patience_lr_scheduler)\n",
    "    hist, t = train_model(m, train_loader, val_loader, crit, opt, sch, f\"Ensemble_{i+1}\", num_epochs=max(10, Config.epochs//Config.num_ensemble_models))\n",
    "    torch.save(m.state_dict(), os.path.join(Config.output_dir, f\"ensemble_member_{i+1}.pth\"))\n",
    "    return m, t, hist\n",
    "\n",
    "def evaluate_deep_ensemble(models_list: List[nn.Module], loader):\n",
    "    all_prob=[]; all_y=[]; t0=time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _ in tqdm(loader, desc=\"Evaluating Ensemble\"):\n",
    "            xb=xb.to(Config.device)\n",
    "            mem_probs=[]\n",
    "            for m in models_list:\n",
    "                m.eval()\n",
    "                logits, _, _ = m(xb)\n",
    "                mem_probs.append(torch.softmax(logits, dim=1).unsqueeze(0))\n",
    "            mean_prob=torch.cat(mem_probs,0).mean(0)\n",
    "            all_prob.append(mean_prob.cpu()); all_y.append(torch.as_tensor(yb).cpu())\n",
    "    t1=time.time()\n",
    "    prob=torch.cat(all_prob,0); y=torch.cat(all_y,0)\n",
    "    pred=prob.argmax(1)\n",
    "    acc=accuracy_score(y.numpy(), pred.numpy())\n",
    "    prec=precision_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    rec=recall_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    f1=f1_score(y.numpy(), pred.numpy(), average='macro', zero_division=0)\n",
    "    ece,brier=calculate_calibration_metrics(prob, y)\n",
    "    plot_reliability_diagram(prob.max(1)[0], (pred==y), ece, 'Deep_Ensemble')\n",
    "    pp = compute_ppdrq_from_logits(torch.log(prob+1e-8), Config.num_classes)\n",
    "    print(f\"\\n[Ensemble] Acc {acc:.4f} F1 {f1:.4f} PPDRQ {pp.mean().item():.4f} ECE {ece:.4f}\")\n",
    "    return {'accuracy':acc,'precision':prec,'recall':rec,'f1_score':f1,\n",
    "            'mean_ppdrq':pp.mean().item(),'ece':ece,'brier_score':brier,\n",
    "            'inference_time':t1-t0}\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Enhanced Paper-Ready Reporting with Professional Visualizations\n",
    "# -------------------------------\n",
    "def save_results_table(results: Dict[str, Dict], times: Dict[str, float], path_csv: str):\n",
    "    \"\"\"Enhanced results table with methodological transparency\"\"\"\n",
    "    rows = []\n",
    "    for name, m in results.items():\n",
    "        train_t = times.get(name, times.get(f\"{name}_Train\", np.nan))\n",
    "        infer_t = m.get('inference_time', times.get(f\"{name}_Infer\", np.nan))\n",
    "        rows.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': m['accuracy'], \n",
    "            'Precision': m['precision'], \n",
    "            'Recall': m['recall'], \n",
    "            'F1_Score': m['f1_score'],\n",
    "            'Mean_PPDRQ': m['mean_ppdrq'], \n",
    "            'ECE': m['ece'], \n",
    "            'Brier_Score': m['brier_score'],\n",
    "            'Train_Time_s': train_t, \n",
    "            'Inference_Time_s': infer_t,\n",
    "            'Params_Used': f\"λ={Config.lambda_triplet}, εp={Config.epsilon_p}, εn={Config.epsilon_n}\"\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    csv_path = os.path.join(Config.output_dir, path_csv)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved enhanced results table -> {csv_path}\")\n",
    "    return df\n",
    "\n",
    "def plot_enhanced_metric_bars(df: pd.DataFrame, metric: str, fname: str):\n",
    "    \"\"\"Professional metric comparison with enhanced styling\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Sort by metric value\n",
    "    df_sorted = df.sort_values(metric, ascending=False)\n",
    "    models = df_sorted['Model'].values\n",
    "    values = df_sorted[metric].values\n",
    "    \n",
    "    # Create bars with model-specific colors\n",
    "    colors = [MODEL_COLORS.get(model, COLORS['primary']) for model in models]\n",
    "    bars = ax.bar(models, values, color=colors, alpha=0.8, \n",
    "                  edgecolor='white', linewidth=2)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + max(values)*0.01, \n",
    "               f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title(), fontweight='bold', fontsize=14)\n",
    "    ax.set_title(f'Model Comparison: {metric.replace(\"_\", \" \").title()}', \n",
    "                fontweight='bold', fontsize=16, pad=20)\n",
    "    ax.tick_params(axis='x', rotation=25, labelsize=12)\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Add subtle background\n",
    "    ax.set_facecolor('#f8f9fa')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(Config.output_dir, fname)\n",
    "    plt.savefig(out, dpi=Config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved enhanced {metric} bar plot -> {out}\")\n",
    "\n",
    "def plot_enhanced_cost_performance(df: pd.DataFrame, perf_metric: str='Accuracy', \n",
    "                                 cost_metric: str='Inference_Time_s', fname: str='cost_perf.png'):\n",
    "    \"\"\"Professional cost-performance analysis\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    x = df[cost_metric].values\n",
    "    y = df[perf_metric].values\n",
    "    models = df['Model'].values\n",
    "    \n",
    "    # Create scatter plot with model-specific colors\n",
    "    for i, model in enumerate(models):\n",
    "        color = MODEL_COLORS.get(model, COLORS['primary'])\n",
    "        ax.scatter(x[i], y[i], c=color, s=200, alpha=0.8, \n",
    "                  edgecolors='white', linewidth=2, label=model)\n",
    "        \n",
    "        # Add model labels with offset\n",
    "        ax.annotate(model, (x[i], y[i]), xytext=(5, 5), \n",
    "                   textcoords='offset points', fontweight='bold',\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.8))\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xlabel(cost_metric.replace('_', ' ').title(), fontweight='bold', fontsize=14)\n",
    "    ax.set_ylabel(perf_metric.replace('_', ' ').title(), fontweight='bold', fontsize=14)\n",
    "    ax.set_title(f'Performance vs Cost Trade-off\\n({perf_metric} vs {cost_metric})', \n",
    "                fontweight='bold', fontsize=16, pad=20)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(Config.output_dir, fname)\n",
    "    plt.savefig(out, dpi=Config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved enhanced cost-performance plot -> {out}\")\n",
    "\n",
    "def enhanced_lambda_sensitivity_sweep(lambdas=(0.0, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 1.0)):\n",
    "    \"\"\"Enhanced λ sensitivity analysis with comprehensive visualization\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LAMBDA SENSITIVITY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Testing different values of λ (triplet loss weight)\")\n",
    "    print(f\"λ values to test: {lambdas}\")\n",
    "    print(f\"Fixed parameters: εp={Config.epsilon_p}, εn={Config.epsilon_n}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results_data = []\n",
    "    acc_list = []\n",
    "    ece_list = []\n",
    "    f1_list = []\n",
    "    brier_list = []\n",
    "    train_times = []\n",
    "    \n",
    "    for lam in lambdas:\n",
    "        print(f\"\\n[λ-sweep] Training with λ={lam}\")\n",
    "        model = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device)\n",
    "        crit = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=lam)\n",
    "        opt = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                         lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "        sch = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.2, \n",
    "                                                  patience=Config.patience_lr_scheduler)\n",
    "        \n",
    "        # Quick training for sensitivity analysis\n",
    "        epochs = max(5, Config.epochs//6)\n",
    "        hist, train_time = train_model(model, train_loader, val_loader, crit, opt, sch, \n",
    "                                     f'Lambda_{lam}', num_epochs=epochs)\n",
    "        res = evaluate_model(model, test_loader, f'Lambda_{lam}')\n",
    "        \n",
    "        # Store results\n",
    "        acc_list.append(res['accuracy'])\n",
    "        ece_list.append(res['ece'])\n",
    "        f1_list.append(res['f1_score'])\n",
    "        brier_list.append(res['brier_score'])\n",
    "        train_times.append(train_time)\n",
    "        \n",
    "        results_data.append({\n",
    "            'Lambda': lam,\n",
    "            'Accuracy': res['accuracy'],\n",
    "            'F1_Score': res['f1_score'],\n",
    "            'ECE': res['ece'],\n",
    "            'Brier_Score': res['brier_score'],\n",
    "            'Train_Time': train_time\n",
    "        })\n",
    "    \n",
    "    # Create comprehensive sensitivity plot\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    lambda_vals = list(lambdas)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax1.plot(lambda_vals, acc_list, marker='o', linewidth=3, markersize=8, \n",
    "             color=COLORS['primary'], label='Accuracy')\n",
    "    ax1.set_xlabel('λ (Triplet Loss Weight)', fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy', fontweight='bold')\n",
    "    ax1.set_title('Accuracy vs λ', fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # ECE plot\n",
    "    ax2.plot(lambda_vals, ece_list, marker='s', linewidth=3, markersize=8, \n",
    "             color=COLORS['secondary'], label='ECE')\n",
    "    ax2.set_xlabel('λ (Triplet Loss Weight)', fontweight='bold')\n",
    "    ax2.set_ylabel('Expected Calibration Error', fontweight='bold')\n",
    "    ax2.set_title('ECE vs λ', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # F1 Score plot\n",
    "    ax3.plot(lambda_vals, f1_list, marker='^', linewidth=3, markersize=8, \n",
    "             color=COLORS['accent'], label='F1 Score')\n",
    "    ax3.set_xlabel('λ (Triplet Loss Weight)', fontweight='bold')\n",
    "    ax3.set_ylabel('F1 Score', fontweight='bold')\n",
    "    ax3.set_title('F1 Score vs λ', fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Brier Score plot\n",
    "    ax4.plot(lambda_vals, brier_list, marker='d', linewidth=3, markersize=8, \n",
    "             color=COLORS['success'], label='Brier Score')\n",
    "    ax4.set_xlabel('λ (Triplet Loss Weight)', fontweight='bold')\n",
    "    ax4.set_ylabel('Brier Score', fontweight='bold')\n",
    "    ax4.setTitle = ax4.set_title('Brier Score vs λ', fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.suptitle('Comprehensive λ Sensitivity Analysis\\n' + \n",
    "                f'εp={Config.epsilon_p}, εn={Config.epsilon_n}', \n",
    "                fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    out = os.path.join(Config.output_dir, 'enhanced_lambda_sensitivity.png')\n",
    "    plt.savefig(out, dpi=Config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved enhanced λ sensitivity plot -> {out}\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    sensitivity_df = pd.DataFrame(results_data)\n",
    "    sensitivity_csv = os.path.join(Config.output_dir, 'lambda_sensitivity_results.csv')\n",
    "    sensitivity_df.to_csv(sensitivity_csv, index=False)\n",
    "    print(f\"Saved λ sensitivity results -> {sensitivity_csv}\")\n",
    "    \n",
    "    # Find optimal lambda\n",
    "    best_lambda_idx = np.argmax(acc_list)\n",
    "    best_lambda = lambdas[best_lambda_idx]\n",
    "    best_acc = acc_list[best_lambda_idx]\n",
    "    \n",
    "    print(f\"\\nOptimal λ: {best_lambda} (Accuracy: {best_acc:.4f})\")\n",
    "    \n",
    "    return sensitivity_df\n",
    "\n",
    "def create_methodological_transparency_report():\n",
    "    \"\"\"Generate comprehensive methodological transparency report\"\"\"\n",
    "    report_content = f\"\"\"\n",
    "# METHODOLOGICAL TRANSPARENCY REPORT\n",
    "Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Hyperparameter Settings\n",
    "\n",
    "### PPDRQ-Specific Parameters:\n",
    "- λ (lambda_triplet): {Config.lambda_triplet}\n",
    "  * Weight for triplet loss component\n",
    "  * Controls balance between classification and embedding quality\n",
    "  \n",
    "- εp (epsilon_p): {Config.epsilon_p}\n",
    "  * Threshold for positive sample selection in triplet loss\n",
    "  * Samples with PPDRQ difference < εp considered similar\n",
    "  \n",
    "- εn (epsilon_n): {Config.epsilon_n}\n",
    "  * Threshold for negative sample selection in triplet loss\n",
    "  * Samples with PPDRQ difference ≥ εn considered dissimilar\n",
    "\n",
    "### Training Parameters:\n",
    "- Learning Rate: {Config.learning_rate}\n",
    "- Weight Decay: {Config.weight_decay}\n",
    "- Batch Size: {Config.batch_size}\n",
    "- Max Epochs: {Config.epochs}\n",
    "- Label Smoothing: {Config.label_smoothing}\n",
    "- Gradient Clipping: {Config.grad_clip}\n",
    "\n",
    "### Regularization:\n",
    "- Mixup Alpha: {Config.mixup_alpha}\n",
    "- CutMix Alpha: {Config.cutmix_alpha}\n",
    "- Dropout Rate: 0.6 (in CustomInceptionV3)\n",
    "\n",
    "### Evaluation Parameters:\n",
    "- MC-Dropout Passes: {Config.mc_dropout_passes}\n",
    "- Ensemble Members: {Config.num_ensemble_models}\n",
    "- Calibration Bins: 10\n",
    "\n",
    "## Model Architecture:\n",
    "- Base: InceptionV3 (pretrained on ImageNet)\n",
    "- Fine-tuning: Last blocks only (Mixed_7x layers)\n",
    "- Custom Head: 2048 → BN → Dropout → Linear(num_classes)\n",
    "- Embedding Layers: 2048 → 1024 → 512\n",
    "\n",
    "## Data Augmentation:\n",
    "- RandomResizedCrop(scale=(0.8, 1.0), ratio=(0.9, 1.1))\n",
    "- RandomRotation(10°)\n",
    "- RandomHorizontalFlip(p=0.5)\n",
    "- GaussianBlur(kernel_size=3)\n",
    "- RandomErasing(p=0.25)\n",
    "\n",
    "## Computational Environment:\n",
    "- Device: {Config.device}\n",
    "- Image Size: {Config.image_size}x{Config.image_size}\n",
    "- Number of Classes: {Config.num_classes}\n",
    "\"\"\"\n",
    "    \n",
    "    report_path = os.path.join(Config.output_dir, 'methodological_transparency_report.txt')\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report_content)\n",
    "    print(f\"Saved methodological transparency report -> {report_path}\")\n",
    "\n",
    "def create_enhanced_timing_analysis(results: Dict, times: Dict):\n",
    "    \"\"\"Create comprehensive timing analysis with professional visualization\"\"\"\n",
    "    # Prepare data\n",
    "    timing_data = []\n",
    "    for model_name, metrics in results.items():\n",
    "        train_time = times.get(model_name, times.get(f\"{model_name}_Train\", 0))\n",
    "        infer_time = metrics.get('inference_time', times.get(f\"{model_name}_Infer\", 0))\n",
    "        accuracy = metrics.get('accuracy', 0)\n",
    "        \n",
    "        timing_data.append({\n",
    "            'Model': model_name,\n",
    "            'Training_Time_s': train_time,\n",
    "            'Inference_Time_s': infer_time,\n",
    "            'Total_Time_s': train_time + infer_time,\n",
    "            'Accuracy': accuracy,\n",
    "            'Time_per_Accuracy': (train_time + infer_time) / max(accuracy, 0.001)\n",
    "        })\n",
    "    \n",
    "    timing_df = pd.DataFrame(timing_data)\n",
    "    \n",
    "    # Create comprehensive timing visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    models = timing_df['Model']\n",
    "    colors = [MODEL_COLORS.get(model, COLORS['primary']) for model in models]\n",
    "    \n",
    "    # Training time comparison\n",
    "    bars1 = ax1.bar(models, timing_df['Training_Time_s'], color=colors, alpha=0.8)\n",
    "    ax1.set_ylabel('Training Time (seconds)', fontweight='bold')\n",
    "    ax1.set_title('Training Time Comparison', fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    for bar, time_val in zip(bars1, timing_df['Training_Time_s']):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(timing_df['Training_Time_s'])*0.01,\n",
    "                f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Inference time comparison\n",
    "    bars2 = ax2.bar(models, timing_df['Inference_Time_s'], color=colors, alpha=0.8)\n",
    "    ax2.set_ylabel('Inference Time (seconds)', fontweight='bold')\n",
    "    ax2.set_title('Inference Time Comparison', fontweight='bold')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    for bar, time_val in zip(bars2, timing_df['Inference_Time_s']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(timing_df['Inference_Time_s'])*0.01,\n",
    "                f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Efficiency plot (Time per Accuracy unit)\n",
    "    bars3 = ax3.bar(models, timing_df['Time_per_Accuracy'], color=colors, alpha=0.8)\n",
    "    ax3.set_ylabel('Time per Accuracy Unit (s)', fontweight='bold')\n",
    "    ax3.set_title('Model Efficiency (Lower is Better)', fontweight='bold')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    for bar, eff_val in zip(bars3, timing_df['Time_per_Accuracy']):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + max(timing_df['Time_per_Accuracy'])*0.01,\n",
    "                f'{eff_val:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Scatter plot: Accuracy vs Total Time\n",
    "    for i, model in enumerate(models):\n",
    "        ax4.scatter(timing_df.iloc[i]['Total_Time_s'], timing_df.iloc[i]['Accuracy'], \n",
    "                   c=colors[i], s=200, alpha=0.8, edgecolors='white', linewidth=2)\n",
    "        ax4.annotate(model, (timing_df.iloc[i]['Total_Time_s'], timing_df.iloc[i]['Accuracy']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.8))\n",
    "    \n",
    "    ax4.set_xlabel('Total Time (seconds)', fontweight='bold')\n",
    "    ax4.set_ylabel('Accuracy', fontweight='bold')\n",
    "    ax4.set_title('Accuracy vs Total Computational Cost', fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Comprehensive Timing Analysis', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    out = os.path.join(Config.output_dir, 'enhanced_timing_analysis.png')\n",
    "    plt.savefig(out, dpi=Config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved enhanced timing analysis -> {out}\")\n",
    "    \n",
    "    # Save timing data\n",
    "    timing_csv = os.path.join(Config.output_dir, 'comprehensive_timing_analysis.csv')\n",
    "    timing_df.to_csv(timing_csv, index=False)\n",
    "    print(f\"Saved timing analysis data -> {timing_csv}\")\n",
    "    \n",
    "    return timing_df\n",
    "\n",
    "# -------------------------------\n",
    "# 10. Enhanced Visualization Functions\n",
    "# -------------------------------\n",
    "CLASS_NAMES = ['AD','MCI','NC']\n",
    "\n",
    "def collect_predictions(model, loader):\n",
    "    y_true=[]; y_pred=[]; conf=[]; probs=[]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _ in loader:\n",
    "            xb = xb.to(Config.device); yb = torch.as_tensor(yb, device=Config.device)\n",
    "            logits, _, _ = model(xb)\n",
    "            p = torch.softmax(logits, dim=1)\n",
    "            pr = p.argmax(1)\n",
    "            y_true.append(yb.cpu()); y_pred.append(pr.cpu()); conf.append(p.max(1)[0].cpu()); probs.append(p.cpu())\n",
    "    return torch.cat(y_true).numpy(), torch.cat(y_pred).numpy(), torch.cat(conf).numpy(), torch.cat(probs,0).numpy()\n",
    "\n",
    "def plot_enhanced_confusion(y_true, y_pred, classes, title, fname, normalize=True):\n",
    "    \"\"\"Enhanced confusion matrix with professional styling\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(classes))))\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True).clip(min=1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Create custom colormap\n",
    "    cmap = plt.cm.Blues\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_title(title, fontweight='bold', fontsize=16, pad=20)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_xticklabels(classes, fontsize=12, fontweight='bold')\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(classes, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            text_val = f\"{cm[i, j]:.3f}\" if normalize else f\"{int(cm[i, j])}\"\n",
    "            ax.text(j, i, text_val, ha=\"center\", va=\"center\", fontweight='bold',\n",
    "                   color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=14)\n",
    "    \n",
    "    ax.set_ylabel('True Label', fontweight='bold', fontsize=14)\n",
    "    ax.set_xlabel('Predicted Label', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(Config.output_dir, fname)\n",
    "    plt.savefig(out, dpi=Config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved enhanced confusion matrix -> {out}\")\n",
    "\n",
    "def export_per_class_report(y_true, y_pred, classes, fname_csv):\n",
    "    rep = classification_report(y_true, y_pred, target_names=classes, output_dict=True, zero_division=0)\n",
    "    df = pd.DataFrame(rep).transpose()\n",
    "    out = os.path.join(Config.output_dir, fname_csv)\n",
    "    df.to_csv(out)\n",
    "    print(f\"Saved per-class report -> {out}\")\n",
    "\n",
    "def enhanced_reliability_grid(models_dict, loader, fname='enhanced_reliability_grid.png', num_bins=10):\n",
    "    \"\"\"Enhanced side-by-side reliability diagrams\"\"\"\n",
    "    cols = len(models_dict)\n",
    "    fig, axes = plt.subplots(1, cols, figsize=(6*cols, 6))\n",
    "    if cols == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (name, model) in enumerate(models_dict.items()):\n",
    "        model.eval()\n",
    "        all_prob = []\n",
    "        all_y = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _ in loader:\n",
    "                xb = xb.to(Config.device)\n",
    "                yb = torch.as_tensor(yb, device=Config.device)\n",
    "                logits, _, _ = model(xb)\n",
    "                pr = torch.softmax(logits, dim=1)\n",
    "                all_prob.append(pr.cpu())\n",
    "                all_y.append(yb.cpu())\n",
    "        \n",
    "        prob = torch.cat(all_prob, 0)\n",
    "        y = torch.cat(all_y, 0)\n",
    "        pred = prob.argmax(1)\n",
    "        conf = prob.max(1)[0]\n",
    "        correct = (pred == y)\n",
    "        \n",
    "        # Compute ECE for title\n",
    "        ece, _ = calculate_calibration_metrics(prob, y)\n",
    "        \n",
    "        # Create bins\n",
    "        bins = np.linspace(0, 1, num_bins + 1)\n",
    "        mids = (bins[:-1] + bins[1:]) / 2\n",
    "        bin_acc = []\n",
    "        counts = []\n",
    "        \n",
    "        for i in range(num_bins):\n",
    "            m = (conf >= bins[i]) & (conf <= bins[i + 1])\n",
    "            if m.sum() > 0:\n",
    "                bin_acc.append(correct[m].float().mean().item())\n",
    "                counts.append(int(m.sum()))\n",
    "            else:\n",
    "                bin_acc.append(0.0)\n",
    "                counts.append(0)\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Perfect calibration line\n",
    "        ax.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.7, label='Perfect Calibration')\n",
    "        \n",
    "        # Bars with model-specific color\n",
    "        color = MODEL_COLORS.get(name, COLORS['primary'])\n",
    "        bars = ax.bar(mids, bin_acc, width=1/num_bins*0.8, alpha=0.8, \n",
    "                     color=color, edgecolor='white', linewidth=1.5)\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "            if count > 0:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.02, \n",
    "                       str(count), ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "        \n",
    "        ax.set_title(f'{name}\\nECE = {ece:.3f}', fontweight='bold', fontsize=12)\n",
    "        ax.set_xlabel('Confidence', fontweight='bold')\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('Accuracy', fontweight='bold')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(loc='upper left', fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Reliability Comparison Across Models', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    out = os.path.join(Config.output_dir, fname)\n",
    "    plt.savefig(out, dpi=Config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved enhanced reliability grid -> {out}\")\n",
    "\n",
    "def plot_enhanced_training_curves(histories_dict, fname='enhanced_training_curves.png'):\n",
    "    \"\"\"Plot enhanced training curves for multiple models\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Define metrics to plot\n",
    "    metrics = [\n",
    "        ('train_acc', 'val_acc', 'Accuracy', axes[0, 0]),\n",
    "        ('train_loss', 'val_loss', 'Loss', axes[0, 1]),\n",
    "        ('train_ppdrq', 'val_ppdrq', 'PPDRQ', axes[1, 0])\n",
    "    ]\n",
    "    \n",
    "    for train_metric, val_metric, title, ax in metrics:\n",
    "        for model_name, history in histories_dict.items():\n",
    "            if train_metric in history and val_metric in history:\n",
    "                color = MODEL_COLORS.get(model_name, COLORS['primary'])\n",
    "                epochs = range(1, len(history[train_metric]) + 1)\n",
    "                \n",
    "                ax.plot(epochs, history[train_metric], '-', color=color, alpha=0.7,\n",
    "                       linewidth=2, label=f'{model_name} Train')\n",
    "                ax.plot(epochs, history[val_metric], '--', color=color, alpha=0.9,\n",
    "                       linewidth=2, label=f'{model_name} Val')\n",
    "        \n",
    "        ax.set_xlabel('Epoch', fontweight='bold')\n",
    "        ax.set_ylabel(title, fontweight='bold')\n",
    "        ax.set_title(f'{title} Curves', fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Summary statistics in the fourth subplot\n",
    "    ax = axes[1, 1]\n",
    "    summary_data = []\n",
    "    for model_name, history in histories_dict.items():\n",
    "        if 'val_acc' in history:\n",
    "            final_acc = history['val_acc'][-1] if history['val_acc'] else 0\n",
    "            best_acc = max(history['val_acc']) if history['val_acc'] else 0\n",
    "            epochs_trained = history.get('epochs_trained', len(history.get('val_acc', [])))\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Model': model_name,\n",
    "                'Final_Val_Acc': final_acc,\n",
    "                'Best_Val_Acc': best_acc,\n",
    "                'Epochs_Trained': epochs_trained\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        models = summary_df['Model']\n",
    "        colors = [MODEL_COLORS.get(model, COLORS['primary']) for model in models]\n",
    "        \n",
    "        bars = ax.bar(models, summary_df['Best_Val_Acc'], color=colors, alpha=0.8)\n",
    "        ax.set_ylabel('Best Validation Accuracy', fontweight='bold')\n",
    "        ax.set_title('Training Summary', fontweight='bold')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        for bar, val in zip(bars, summary_df['Best_Val_Acc']):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                   f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Enhanced Training Analysis', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    out = os.path.join(Config.output_dir, fname)\n",
    "    plt.savefig(out, dpi=Config.FIGURE_DPI, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved enhanced training curves -> {out}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 11. Main Execution with Enhanced Features\n",
    "# -------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Create methodological transparency report\n",
    "    create_methodological_transparency_report()\n",
    "    \n",
    "    results: Dict[str, Dict] = {}\n",
    "    times: Dict[str, float] = {}\n",
    "    histories: Dict[str, Dict] = {}\n",
    "\n",
    "    # Baseline (with label smoothing + anti-overfitting)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING BASELINE MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    baseline = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device)\n",
    "    crit_base = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=0.0)  # CE only\n",
    "    opt_base = optim.AdamW(filter(lambda p: p.requires_grad, baseline.parameters()), \n",
    "                          lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    sch_base = optim.lr_scheduler.ReduceLROnPlateau(opt_base, mode='min', factor=0.2, \n",
    "                                                   patience=Config.patience_lr_scheduler)\n",
    "    hist_b, t_b = train_model(baseline, train_loader, val_loader, crit_base, opt_base, sch_base, 'Baseline')\n",
    "    res_b = evaluate_model(baseline, test_loader, 'Baseline')\n",
    "    results['Baseline'] = res_b\n",
    "    times['Baseline'] = t_b\n",
    "    histories['Baseline'] = hist_b\n",
    "\n",
    "    # PPDRQ-weighted CE only\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING PPDRQ-WEIGHTED MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    class PPDRQWeightedCE(nn.Module):\n",
    "        def __init__(self, num_classes):\n",
    "            super().__init__()\n",
    "            self.num_classes = num_classes\n",
    "        def forward(self, logits, _emb, labels, **kwargs):\n",
    "            pp = compute_ppdrq_from_logits(logits, self.num_classes)\n",
    "            per_ce = F.cross_entropy(logits, labels, reduction='none', label_smoothing=Config.label_smoothing)\n",
    "            w = 1 + (1 - pp)\n",
    "            loss = torch.mean(per_ce * w)\n",
    "            return loss, loss, torch.tensor(0.0, device=logits.device), pp\n",
    "\n",
    "    ppdrq_model = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device)\n",
    "    crit_pp = PPDRQWeightedCE(num_classes=Config.num_classes)\n",
    "    opt_pp = optim.AdamW(filter(lambda p: p.requires_grad, ppdrq_model.parameters()), \n",
    "                        lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    sch_pp = optim.lr_scheduler.ReduceLROnPlateau(opt_pp, mode='min', factor=0.2, \n",
    "                                                 patience=Config.patience_lr_scheduler)\n",
    "    hist_p, t_p = train_model(ppdrq_model, train_loader, val_loader, crit_pp, opt_pp, sch_pp, 'PPDRQ_CE')\n",
    "    res_p = evaluate_model(ppdrq_model, test_loader, 'PPDRQ_CE')\n",
    "    results['PPDRQ_CE'] = res_p\n",
    "    times['PPDRQ_CE'] = t_p\n",
    "    histories['PPDRQ_CE'] = hist_p\n",
    "\n",
    "    # Triplet model\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING TRIPLET MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    triplet_model = CustomInceptionV3(num_classes=Config.num_classes).to(Config.device)\n",
    "    crit_trip = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=Config.lambda_triplet)\n",
    "    opt_trip = optim.AdamW(filter(lambda p: p.requires_grad, triplet_model.parameters()), \n",
    "                          lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    sch_trip = optim.lr_scheduler.ReduceLROnPlateau(opt_trip, mode='min', factor=0.2, \n",
    "                                                   patience=Config.patience_lr_scheduler)\n",
    "    hist_t, t_t = train_model(triplet_model, train_loader, val_loader, crit_trip, opt_trip, sch_trip, 'Triplet')\n",
    "    res_t = evaluate_model(triplet_model, test_loader, 'Triplet')\n",
    "    results['Triplet'] = res_t\n",
    "    times['Triplet'] = t_t\n",
    "    histories['Triplet'] = hist_t\n",
    "\n",
    "    # MC-Dropout\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING MC-DROPOUT MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    mc_model = CustomInceptionV3(num_classes=Config.num_classes, dropout_rate=0.6).to(Config.device)\n",
    "    crit_mc = CombinedLoss(num_classes=Config.num_classes, lambda_triplet=0.0)\n",
    "    opt_mc = optim.AdamW(filter(lambda p: p.requires_grad, mc_model.parameters()), \n",
    "                        lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    sch_mc = optim.lr_scheduler.ReduceLROnPlateau(opt_mc, mode='min', factor=0.2, \n",
    "                                                 patience=Config.patience_lr_scheduler)\n",
    "    hist_m, t_m = train_model(mc_model, train_loader, val_loader, crit_mc, opt_mc, sch_mc, 'MC_Dropout')\n",
    "    res_m = mc_dropout_predict(mc_model, test_loader, num_passes=Config.mc_dropout_passes)\n",
    "    results['MC_Dropout'] = res_m\n",
    "    times['MC_Dropout_Train'] = t_m\n",
    "    times['MC_Dropout_Infer'] = res_m['inference_time']\n",
    "    histories['MC_Dropout'] = hist_m\n",
    "\n",
    "    # Deep Ensemble\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING DEEP ENSEMBLE\")\n",
    "    print(\"=\"*60)\n",
    "    members = []\n",
    "    t_mem = []\n",
    "    for i in range(Config.num_ensemble_models):\n",
    "        m, t, hist = train_ensemble_member(i, train_loader, val_loader)\n",
    "        members.append(m)\n",
    "        t_mem.append(t)\n",
    "    res_e = evaluate_deep_ensemble(members, test_loader)\n",
    "    results['Ensemble'] = res_e\n",
    "    times['Ensemble_Train'] = sum(t_mem)\n",
    "    times['Ensemble_Infer'] = res_e['inference_time']\n",
    "\n",
    "    # Enhanced Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Model':<15}{'Acc':>8}{'Prec':>8}{'Rec':>8}{'F1':>8}{'PPDRQ':>9}{'ECE':>8}{'Brier':>8}{'Train(s)':>10}{'Infer(s)':>10}\")\n",
    "    print(\"-\" * 95)\n",
    "    for k, v in results.items():\n",
    "        tr = times.get(k, times.get(f\"{k}_Train\", 0.0))\n",
    "        inf = v.get('inference_time', times.get(f\"{k}_Infer\", 0.0))\n",
    "        print(f\"{k:<15}{v['accuracy']:>8.3f}{v['precision']:>8.3f}{v['recall']:>8.3f}{v['f1_score']:>8.3f}{v['mean_ppdrq']:>9.3f}{v['ece']:>8.3f}{v['brier_score']:>8.3f}{tr:>10.1f}{inf:>10.1f}\")\n",
    "\n",
    "    # Enhanced visualizations and reports\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATING ENHANCED VISUALIZATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Plot enhanced training curves\n",
    "    plot_enhanced_training_curves(histories, 'enhanced_training_curves.png')\n",
    "    \n",
    "    # Layer-wise analysis for main models\n",
    "    for model_name, model in [('Baseline', baseline), ('PPDRQ_CE', ppdrq_model), ('Triplet', triplet_model)]:\n",
    "        get_and_plot_layerwise_ppdrq(model, test_loader, model_name, f\"- Test Set\")\n",
    "\n",
    "    # Unified enhanced table + plots\n",
    "    df = save_results_table(results, times, \"enhanced_results_all_models.csv\")\n",
    "    plot_enhanced_metric_bars(df, \"Accuracy\", \"enhanced_accuracy_comparison.png\")\n",
    "    plot_enhanced_metric_bars(df, \"F1_Score\", \"enhanced_f1_comparison.png\")\n",
    "    plot_enhanced_metric_bars(df, \"ECE\", \"enhanced_ece_comparison.png\")\n",
    "    plot_enhanced_metric_bars(df, \"Brier_Score\", \"enhanced_brier_comparison.png\")\n",
    "    \n",
    "    # Enhanced cost-performance analysis\n",
    "    plot_enhanced_cost_performance(df, perf_metric=\"Accuracy\", cost_metric=\"Inference_Time_s\", \n",
    "                                  fname=\"enhanced_cost_perf_analysis.png\")\n",
    "    \n",
    "    # Comprehensive timing analysis\n",
    "    timing_df = create_enhanced_timing_analysis(results, times)\n",
    "\n",
    "    # Enhanced confusion matrices and reports\n",
    "    print(\"\\nGenerating enhanced confusion matrices and reports...\")\n",
    "    model_objs = {\n",
    "        'Baseline': baseline,\n",
    "        'PPDRQ_CE': ppdrq_model,\n",
    "        'Triplet': triplet_model,\n",
    "        'MC_Dropout': mc_model\n",
    "    }\n",
    "    \n",
    "    # Ensemble wrapper\n",
    "    class EnsembleWrapper(nn.Module):\n",
    "        def __init__(self, members):\n",
    "            super().__init__()\n",
    "            self.members = members\n",
    "        def forward(self, x):\n",
    "            outs = []\n",
    "            for m in self.members:\n",
    "                m.eval()\n",
    "                lo, _, _ = m(x)\n",
    "                outs.append(lo.unsqueeze(0))\n",
    "            logits = torch.mean(torch.cat(outs, 0), dim=0)\n",
    "            return logits, torch.zeros(x.size(0), 512, device=logits.device), {'final_logits': logits}\n",
    "    \n",
    "    ens_wrapper = EnsembleWrapper(members)\n",
    "    model_objs['Ensemble'] = ens_wrapper\n",
    "\n",
    "    for name, m in model_objs.items():\n",
    "        y_t, y_p, confs, prob = collect_predictions(m, test_loader)\n",
    "        plot_enhanced_confusion(y_t, y_p, CLASS_NAMES, f\"Confusion Matrix: {name}\", \n",
    "                               f\"enhanced_cm_{name}.png\", normalize=True)\n",
    "        export_per_class_report(y_t, y_p, CLASS_NAMES, f\"enhanced_per_class_{name}.csv\")\n",
    "\n",
    "    # Enhanced reliability grid\n",
    "    enhanced_reliability_grid(model_objs, test_loader, fname='enhanced_reliability_grid.png', num_bins=10)\n",
    "\n",
    "    # Enhanced λ sensitivity analysis\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONDUCTING ENHANCED λ SENSITIVITY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    sensitivity_df = enhanced_lambda_sensitivity_sweep()\n",
    "\n",
    "    # Unseen domain evaluation if available\n",
    "    if len(unseen_dataset) > 0:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUATING ON UNSEEN DOMAIN\")\n",
    "        print(\"=\"*60)\n",
    "        # Load best weights if present\n",
    "        for tag, model in [('Baseline', baseline), ('PPDRQ_CE', ppdrq_model), \n",
    "                          ('Triplet', triplet_model), ('MC_Dropout', mc_model)]:\n",
    "            pth = os.path.join(Config.output_dir, f\"{tag}_best.pth\")\n",
    "            if os.path.exists(pth):\n",
    "                model.load_state_dict(torch.load(pth, map_location=Config.device))\n",
    "        \n",
    "        # Evaluate on unseen data\n",
    "        res_b_u = evaluate_model(baseline, unseen_loader, 'Baseline_Unseen')\n",
    "        res_p_u = evaluate_model(ppdrq_model, unseen_loader, 'PPDRQ_CE_Unseen')\n",
    "        res_t_u = evaluate_model(triplet_model, unseen_loader, 'Triplet_Unseen')\n",
    "        res_m_u = mc_dropout_predict(mc_model, unseen_loader, num_passes=Config.mc_dropout_passes)\n",
    "        if len(members) == Config.num_ensemble_models:\n",
    "            res_e_u = evaluate_deep_ensemble(members, unseen_loader)\n",
    "        print(\"Unseen domain evaluation complete.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENHANCED ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Generated files include:\")\n",
    "    print(\"- Enhanced visualizations with professional styling\")\n",
    "    print(\"- Comprehensive methodological transparency report\")\n",
    "    print(\"- Detailed timing analysis\")\n",
    "    print(\"- λ sensitivity analysis with multiple metrics\")\n",
    "    print(\"- Enhanced confusion matrices and reliability diagrams\")\n",
    "    print(\"- Publication-ready figures with high DPI\")\n",
    "    print(f\"\\nAll results saved to: {Config.output_dir}/\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
